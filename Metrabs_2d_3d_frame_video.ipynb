{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73071db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Generated 3D frame for frame 0\n",
      "Generated 3D frame for frame 10\n",
      "Generated 3D frame for frame 20\n",
      "Generated 3D frame for frame 30\n",
      "Generated 3D frame for frame 40\n",
      "Generated 3D frame for frame 50\n",
      "Generated 3D frame for frame 60\n",
      "Generated 3D frame for frame 70\n",
      "Generated 3D frame for frame 80\n",
      "Generated 3D frame for frame 90\n",
      "Generated 3D frame for frame 100\n",
      "Generated 3D frame for frame 110\n",
      "Generated 3D frame for frame 120\n",
      "No pose detected in frame 130\n",
      "Generated 3D frame for frame 140\n",
      "Wrote frame 0 to 3D video (from temp_3d_frames\\3d_frame_0.png)\n",
      "Wrote frame 1 to 3D video (from temp_3d_frames\\3d_frame_0.png)\n",
      "Wrote frame 2 to 3D video (from temp_3d_frames\\3d_frame_0.png)\n",
      "Wrote frame 3 to 3D video (from temp_3d_frames\\3d_frame_0.png)\n",
      "Wrote frame 4 to 3D video (from temp_3d_frames\\3d_frame_0.png)\n",
      "Wrote frame 5 to 3D video (from temp_3d_frames\\3d_frame_0.png)\n",
      "Wrote frame 6 to 3D video (from temp_3d_frames\\3d_frame_0.png)\n",
      "Wrote frame 7 to 3D video (from temp_3d_frames\\3d_frame_0.png)\n",
      "Wrote frame 8 to 3D video (from temp_3d_frames\\3d_frame_0.png)\n",
      "Wrote frame 9 to 3D video (from temp_3d_frames\\3d_frame_0.png)\n",
      "Wrote frame 10 to 3D video (from temp_3d_frames\\3d_frame_10.png)\n",
      "Wrote frame 11 to 3D video (from temp_3d_frames\\3d_frame_10.png)\n",
      "Wrote frame 12 to 3D video (from temp_3d_frames\\3d_frame_10.png)\n",
      "Wrote frame 13 to 3D video (from temp_3d_frames\\3d_frame_10.png)\n",
      "Wrote frame 14 to 3D video (from temp_3d_frames\\3d_frame_10.png)\n",
      "Wrote frame 15 to 3D video (from temp_3d_frames\\3d_frame_10.png)\n",
      "Wrote frame 16 to 3D video (from temp_3d_frames\\3d_frame_10.png)\n",
      "Wrote frame 17 to 3D video (from temp_3d_frames\\3d_frame_10.png)\n",
      "Wrote frame 18 to 3D video (from temp_3d_frames\\3d_frame_10.png)\n",
      "Wrote frame 19 to 3D video (from temp_3d_frames\\3d_frame_10.png)\n",
      "Wrote frame 20 to 3D video (from temp_3d_frames\\3d_frame_20.png)\n",
      "Wrote frame 21 to 3D video (from temp_3d_frames\\3d_frame_20.png)\n",
      "Wrote frame 22 to 3D video (from temp_3d_frames\\3d_frame_20.png)\n",
      "Wrote frame 23 to 3D video (from temp_3d_frames\\3d_frame_20.png)\n",
      "Wrote frame 24 to 3D video (from temp_3d_frames\\3d_frame_20.png)\n",
      "Wrote frame 25 to 3D video (from temp_3d_frames\\3d_frame_20.png)\n",
      "Wrote frame 26 to 3D video (from temp_3d_frames\\3d_frame_20.png)\n",
      "Wrote frame 27 to 3D video (from temp_3d_frames\\3d_frame_20.png)\n",
      "Wrote frame 28 to 3D video (from temp_3d_frames\\3d_frame_20.png)\n",
      "Wrote frame 29 to 3D video (from temp_3d_frames\\3d_frame_20.png)\n",
      "Wrote frame 30 to 3D video (from temp_3d_frames\\3d_frame_30.png)\n",
      "Wrote frame 31 to 3D video (from temp_3d_frames\\3d_frame_30.png)\n",
      "Wrote frame 32 to 3D video (from temp_3d_frames\\3d_frame_30.png)\n",
      "Wrote frame 33 to 3D video (from temp_3d_frames\\3d_frame_30.png)\n",
      "Wrote frame 34 to 3D video (from temp_3d_frames\\3d_frame_30.png)\n",
      "Wrote frame 35 to 3D video (from temp_3d_frames\\3d_frame_30.png)\n",
      "Wrote frame 36 to 3D video (from temp_3d_frames\\3d_frame_30.png)\n",
      "Wrote frame 37 to 3D video (from temp_3d_frames\\3d_frame_30.png)\n",
      "Wrote frame 38 to 3D video (from temp_3d_frames\\3d_frame_30.png)\n",
      "Wrote frame 39 to 3D video (from temp_3d_frames\\3d_frame_30.png)\n",
      "Wrote frame 40 to 3D video (from temp_3d_frames\\3d_frame_40.png)\n",
      "Wrote frame 41 to 3D video (from temp_3d_frames\\3d_frame_40.png)\n",
      "Wrote frame 42 to 3D video (from temp_3d_frames\\3d_frame_40.png)\n",
      "Wrote frame 43 to 3D video (from temp_3d_frames\\3d_frame_40.png)\n",
      "Wrote frame 44 to 3D video (from temp_3d_frames\\3d_frame_40.png)\n",
      "Wrote frame 45 to 3D video (from temp_3d_frames\\3d_frame_40.png)\n",
      "Wrote frame 46 to 3D video (from temp_3d_frames\\3d_frame_40.png)\n",
      "Wrote frame 47 to 3D video (from temp_3d_frames\\3d_frame_40.png)\n",
      "Wrote frame 48 to 3D video (from temp_3d_frames\\3d_frame_40.png)\n",
      "Wrote frame 49 to 3D video (from temp_3d_frames\\3d_frame_40.png)\n",
      "Wrote frame 50 to 3D video (from temp_3d_frames\\3d_frame_50.png)\n",
      "Wrote frame 51 to 3D video (from temp_3d_frames\\3d_frame_50.png)\n",
      "Wrote frame 52 to 3D video (from temp_3d_frames\\3d_frame_50.png)\n",
      "Wrote frame 53 to 3D video (from temp_3d_frames\\3d_frame_50.png)\n",
      "Wrote frame 54 to 3D video (from temp_3d_frames\\3d_frame_50.png)\n",
      "Wrote frame 55 to 3D video (from temp_3d_frames\\3d_frame_50.png)\n",
      "Wrote frame 56 to 3D video (from temp_3d_frames\\3d_frame_50.png)\n",
      "Wrote frame 57 to 3D video (from temp_3d_frames\\3d_frame_50.png)\n",
      "Wrote frame 58 to 3D video (from temp_3d_frames\\3d_frame_50.png)\n",
      "Wrote frame 59 to 3D video (from temp_3d_frames\\3d_frame_50.png)\n",
      "Wrote frame 60 to 3D video (from temp_3d_frames\\3d_frame_60.png)\n",
      "Wrote frame 61 to 3D video (from temp_3d_frames\\3d_frame_60.png)\n",
      "Wrote frame 62 to 3D video (from temp_3d_frames\\3d_frame_60.png)\n",
      "Wrote frame 63 to 3D video (from temp_3d_frames\\3d_frame_60.png)\n",
      "Wrote frame 64 to 3D video (from temp_3d_frames\\3d_frame_60.png)\n",
      "Wrote frame 65 to 3D video (from temp_3d_frames\\3d_frame_60.png)\n",
      "Wrote frame 66 to 3D video (from temp_3d_frames\\3d_frame_60.png)\n",
      "Wrote frame 67 to 3D video (from temp_3d_frames\\3d_frame_60.png)\n",
      "Wrote frame 68 to 3D video (from temp_3d_frames\\3d_frame_60.png)\n",
      "Wrote frame 69 to 3D video (from temp_3d_frames\\3d_frame_60.png)\n",
      "Wrote frame 70 to 3D video (from temp_3d_frames\\3d_frame_70.png)\n",
      "Wrote frame 71 to 3D video (from temp_3d_frames\\3d_frame_70.png)\n",
      "Wrote frame 72 to 3D video (from temp_3d_frames\\3d_frame_70.png)\n",
      "Wrote frame 73 to 3D video (from temp_3d_frames\\3d_frame_70.png)\n",
      "Wrote frame 74 to 3D video (from temp_3d_frames\\3d_frame_70.png)\n",
      "Wrote frame 75 to 3D video (from temp_3d_frames\\3d_frame_70.png)\n",
      "Wrote frame 76 to 3D video (from temp_3d_frames\\3d_frame_70.png)\n",
      "Wrote frame 77 to 3D video (from temp_3d_frames\\3d_frame_70.png)\n",
      "Wrote frame 78 to 3D video (from temp_3d_frames\\3d_frame_70.png)\n",
      "Wrote frame 79 to 3D video (from temp_3d_frames\\3d_frame_70.png)\n",
      "Wrote frame 80 to 3D video (from temp_3d_frames\\3d_frame_80.png)\n",
      "Wrote frame 81 to 3D video (from temp_3d_frames\\3d_frame_80.png)\n",
      "Wrote frame 82 to 3D video (from temp_3d_frames\\3d_frame_80.png)\n",
      "Wrote frame 83 to 3D video (from temp_3d_frames\\3d_frame_80.png)\n",
      "Wrote frame 84 to 3D video (from temp_3d_frames\\3d_frame_80.png)\n",
      "Wrote frame 85 to 3D video (from temp_3d_frames\\3d_frame_80.png)\n",
      "Wrote frame 86 to 3D video (from temp_3d_frames\\3d_frame_80.png)\n",
      "Wrote frame 87 to 3D video (from temp_3d_frames\\3d_frame_80.png)\n",
      "Wrote frame 88 to 3D video (from temp_3d_frames\\3d_frame_80.png)\n",
      "Wrote frame 89 to 3D video (from temp_3d_frames\\3d_frame_80.png)\n",
      "Wrote frame 90 to 3D video (from temp_3d_frames\\3d_frame_90.png)\n",
      "Wrote frame 91 to 3D video (from temp_3d_frames\\3d_frame_90.png)\n",
      "Wrote frame 92 to 3D video (from temp_3d_frames\\3d_frame_90.png)\n",
      "Wrote frame 93 to 3D video (from temp_3d_frames\\3d_frame_90.png)\n",
      "Wrote frame 94 to 3D video (from temp_3d_frames\\3d_frame_90.png)\n",
      "Wrote frame 95 to 3D video (from temp_3d_frames\\3d_frame_90.png)\n",
      "Wrote frame 96 to 3D video (from temp_3d_frames\\3d_frame_90.png)\n",
      "Wrote frame 97 to 3D video (from temp_3d_frames\\3d_frame_90.png)\n",
      "Wrote frame 98 to 3D video (from temp_3d_frames\\3d_frame_90.png)\n",
      "Wrote frame 99 to 3D video (from temp_3d_frames\\3d_frame_90.png)\n",
      "Wrote frame 100 to 3D video (from temp_3d_frames\\3d_frame_100.png)\n",
      "Wrote frame 101 to 3D video (from temp_3d_frames\\3d_frame_100.png)\n",
      "Wrote frame 102 to 3D video (from temp_3d_frames\\3d_frame_100.png)\n",
      "Wrote frame 103 to 3D video (from temp_3d_frames\\3d_frame_100.png)\n",
      "Wrote frame 104 to 3D video (from temp_3d_frames\\3d_frame_100.png)\n",
      "Wrote frame 105 to 3D video (from temp_3d_frames\\3d_frame_100.png)\n",
      "Wrote frame 106 to 3D video (from temp_3d_frames\\3d_frame_100.png)\n",
      "Wrote frame 107 to 3D video (from temp_3d_frames\\3d_frame_100.png)\n",
      "Wrote frame 108 to 3D video (from temp_3d_frames\\3d_frame_100.png)\n",
      "Wrote frame 109 to 3D video (from temp_3d_frames\\3d_frame_100.png)\n",
      "Wrote frame 110 to 3D video (from temp_3d_frames\\3d_frame_110.png)\n",
      "Wrote frame 111 to 3D video (from temp_3d_frames\\3d_frame_110.png)\n",
      "Wrote frame 112 to 3D video (from temp_3d_frames\\3d_frame_110.png)\n",
      "Wrote frame 113 to 3D video (from temp_3d_frames\\3d_frame_110.png)\n",
      "Wrote frame 114 to 3D video (from temp_3d_frames\\3d_frame_110.png)\n",
      "Wrote frame 115 to 3D video (from temp_3d_frames\\3d_frame_110.png)\n",
      "Wrote frame 116 to 3D video (from temp_3d_frames\\3d_frame_110.png)\n",
      "Wrote frame 117 to 3D video (from temp_3d_frames\\3d_frame_110.png)\n",
      "Wrote frame 118 to 3D video (from temp_3d_frames\\3d_frame_110.png)\n",
      "Wrote frame 119 to 3D video (from temp_3d_frames\\3d_frame_110.png)\n",
      "Wrote frame 120 to 3D video (from temp_3d_frames\\3d_frame_120.png)\n",
      "Wrote frame 121 to 3D video (from temp_3d_frames\\3d_frame_120.png)\n",
      "Wrote frame 122 to 3D video (from temp_3d_frames\\3d_frame_120.png)\n",
      "Wrote frame 123 to 3D video (from temp_3d_frames\\3d_frame_120.png)\n",
      "Wrote frame 124 to 3D video (from temp_3d_frames\\3d_frame_120.png)\n",
      "Wrote frame 125 to 3D video (from temp_3d_frames\\3d_frame_120.png)\n",
      "Wrote frame 126 to 3D video (from temp_3d_frames\\3d_frame_120.png)\n",
      "Wrote frame 127 to 3D video (from temp_3d_frames\\3d_frame_120.png)\n",
      "Wrote frame 128 to 3D video (from temp_3d_frames\\3d_frame_120.png)\n",
      "Wrote frame 129 to 3D video (from temp_3d_frames\\3d_frame_120.png)\n",
      "Wrote frame 130 to 3D video (from temp_3d_frames\\3d_frame_140.png)\n",
      "Wrote frame 131 to 3D video (from temp_3d_frames\\3d_frame_140.png)\n",
      "Wrote frame 132 to 3D video (from temp_3d_frames\\3d_frame_140.png)\n",
      "Wrote frame 133 to 3D video (from temp_3d_frames\\3d_frame_140.png)\n",
      "Wrote frame 134 to 3D video (from temp_3d_frames\\3d_frame_140.png)\n",
      "Wrote frame 135 to 3D video (from temp_3d_frames\\3d_frame_140.png)\n",
      "Wrote frame 136 to 3D video (from temp_3d_frames\\3d_frame_140.png)\n",
      "Wrote frame 137 to 3D video (from temp_3d_frames\\3d_frame_140.png)\n",
      "Wrote frame 138 to 3D video (from temp_3d_frames\\3d_frame_140.png)\n",
      "Wrote frame 139 to 3D video (from temp_3d_frames\\3d_frame_140.png)\n",
      "Wrote blank frame 140 to 3D video (no corresponding 3D frame)\n",
      "Wrote blank frame 141 to 3D video (no corresponding 3D frame)\n",
      "Wrote blank frame 142 to 3D video (no corresponding 3D frame)\n",
      "Wrote blank frame 143 to 3D video (no corresponding 3D frame)\n",
      "Wrote blank frame 144 to 3D video (no corresponding 3D frame)\n",
      "Processed video saved as output_video3d_withlabelschanged.mp4\n",
      "Pose data saved as 2dand3dposedatawithlabelschanged.xlsx\n",
      "Cleaned up temporary directory temp_3d_frames\n",
      "Computed pixel-to-mm conversion factor: 2.0306 mm/pixel\n",
      "Average 3D hip-to-ankle distance: 414.96 mm\n",
      "Average 2D hip-to-ankle distance: 208.55 pixels\n",
      "Updated pose data with 2D coordinates in millimeters saved as 2dand3dposedatawithlabelschanged_converted.xlsx\n",
      "Detected 1 persons in the video.\n",
      "\n",
      "Processing Person 0...\n",
      "2D Metrics for Person 0:\n",
      "Stride Time Left (s): 1.11\n",
      "Stride Time Right (s): 1.17\n",
      "Stride Length Left (mm): 52.17\n",
      "Stride Length Right (mm): 46.20\n",
      "Cadence (steps/min): 90.00\n",
      "Gait Speed Left (mm/s): 46.96\n",
      "Gait Speed Right (mm/s): 39.60\n",
      "Avg Left Knee Angle (deg): 160.22\n",
      "Avg Right Knee Angle (deg): 159.28\n",
      "Avg Left Hip Angle (deg): 175.54\n",
      "Avg Right Hip Angle (deg): 175.19\n",
      "3D Metrics for Person 0:\n",
      "Stride Time Left (s): 1.00\n",
      "Stride Time Right (s): 1.00\n",
      "Stride Length Left (mm): 149.38\n",
      "Stride Length Right (mm): 149.04\n",
      "Cadence (steps/min): 90.00\n",
      "Gait Speed Left (mm/s): 149.38\n",
      "Gait Speed Right (mm/s): 149.04\n",
      "Avg Left Knee Angle (deg): 83.38\n",
      "Avg Right Knee Angle (deg): 82.39\n",
      "Avg Left Hip Angle (deg): 130.10\n",
      "Avg Right Hip Angle (deg): 126.47\n",
      "\n",
      "Gait metrics for all detected persons saved to 'gait_metrics_per_person.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import urllib.request\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as tfhub\n",
    "import tensorflow_io as tfio\n",
    "import cv2\n",
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial import distance\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "\n",
    "def extract_model(zip_path, extract_to=\"models\"):\n",
    "    \"\"\"Extracts a zip file and returns the extracted directory path.\"\"\"\n",
    "    extract_dir = os.path.join(os.path.dirname(zip_path), extract_to)\n",
    "\n",
    "    if not os.path.exists(extract_dir):\n",
    "        os.makedirs(extract_dir)\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "\n",
    "    model_dir = os.path.join(extract_dir, os.path.splitext(os.path.basename(zip_path))[0])\n",
    "    return model_dir\n",
    "\n",
    "def draw_skeleton(frame, keypoints, skeleton_edges):\n",
    "    \"\"\"Draws skeleton on the frame using detected keypoints.\"\"\"\n",
    "    for (i, j) in skeleton_edges:\n",
    "        pt1, pt2 = keypoints[i], keypoints[j]\n",
    "\n",
    "        # Check if confidence is available (shape [N, 24, 3] expected)\n",
    "        if keypoints.shape[-1] == 3:  \n",
    "            if (pt1[2] > 0.5) and (pt2[2] > 0.5):  # Confidence threshold\n",
    "                cv2.line(frame, (int(pt1[0]), int(pt1[1])), (int(pt2[0]), int(pt2[1])), (255, 0, 0), 2)  # Blue lines\n",
    "        else:  \n",
    "            # Draw without confidence check\n",
    "            cv2.line(frame, (int(pt1[0]), int(pt1[1])), (int(pt2[0]), int(pt2[1])), (255, 0, 0), 2)\n",
    "\n",
    "    return frame\n",
    "\n",
    "def visualize(im, detections, poses3d, poses2d, edges):\n",
    "    \"\"\"Visualize 2D and 3D poses with bounding boxes.\"\"\"\n",
    "    fig = plt.figure(figsize=(10, 5.2))\n",
    "    image_ax = fig.add_subplot(1, 2, 1)\n",
    "    image_ax.imshow(im)\n",
    "    for x, y, w, h in detections[:, :4]:\n",
    "        image_ax.add_patch(Rectangle((x, y), w, h, fill=False))\n",
    "\n",
    "    pose_ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "    pose_ax.view_init(5, -85)\n",
    "\n",
    "    # Collect all coordinates to determine dynamic axis limits\n",
    "    all_x_3d, all_y_3d, all_z_3d = [], [], []\n",
    "    for pose3d in poses3d:\n",
    "        all_x_3d.extend(pose3d[:, 0])\n",
    "        all_y_3d.extend(pose3d[:, 1])\n",
    "        all_z_3d.extend(pose3d[:, 2])\n",
    "\n",
    "    # Set dynamic axis limits with padding\n",
    "    if all_x_3d:\n",
    "        x_min, x_max = min(all_x_3d), max(all_x_3d)\n",
    "        y_min, y_max = min(all_y_3d), max(all_y_3d)\n",
    "        z_min, z_max = min(all_z_3d), max(all_z_3d)\n",
    "        padding = 0.5  # Add padding in meters\n",
    "        pose_ax.set_xlim(x_min - padding, x_max + padding)\n",
    "        pose_ax.set_ylim(y_min - padding, y_max + padding)\n",
    "        pose_ax.set_zlim(z_min - padding, z_max + padding)\n",
    "    else:\n",
    "        pose_ax.set_xlim(-1.5, 1.5)\n",
    "        pose_ax.set_ylim(-1.5, 1.5)\n",
    "        pose_ax.set_zlim(0, 3.0)\n",
    "\n",
    "    pose_ax.set_xlabel('X (m)')\n",
    "    pose_ax.set_ylabel('Y (m)')\n",
    "    pose_ax.set_zlabel('Z (m)')\n",
    "\n",
    "    # Swap Y and Z axes and negate Z for Metrabs coordinate system\n",
    "    for pose3d, pose2d in zip(poses3d, poses2d):\n",
    "        pose3d_adjusted = pose3d.copy()\n",
    "        pose3d_adjusted[:, 1], pose3d_adjusted[:, 2] = pose3d[:, 2], -pose3d[:, 1]\n",
    "        for i_start, i_end in edges:\n",
    "            image_ax.plot(*zip(pose2d[i_start], pose2d[i_end]), marker='o', markersize=2)\n",
    "            pose_ax.plot(*zip(pose3d_adjusted[i_start], pose3d_adjusted[i_end]), marker='o', markersize=2)\n",
    "        image_ax.scatter(*pose2d.T, s=2)\n",
    "        pose_ax.scatter(*pose3d_adjusted.T, s=2)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#def process_video(video_path, model, skeleton_edges, output_video=\"output_video3d_withlabelschanged.mp4\", frame_interval=10, output_excel=\"2dand3dposedatawithlabelschanged.xlsx\"):\n",
    "    \"\"\"Process video frames with pose estimation, overlay skeletons and person labels, visualize 3D poses, and save coordinates to an Excel file.\"\"\"\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_width = int(cap.get(3))\n",
    "    frame_height = int(cap.get(4))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out_2d = cv2.VideoWriter(output_video, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    # Initialize 3D video output\n",
    "    output_3d_video = \"output_3d_video.mp4\"\n",
    "    out_3d = cv2.VideoWriter(output_3d_video, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    frame_count = 0\n",
    "    data_list = []  # Store pose data\n",
    "\n",
    "    # Create temporary directory for 3D plot images\n",
    "    temp_dir = \"temp_3d_frames\"\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if frame_count % frame_interval == 0:\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            original_height, original_width = frame.shape[:2]\n",
    "            frame_rgb_resized = cv2.resize(frame_rgb, (640, 480))\n",
    "            image_tensor = tf.convert_to_tensor(frame_rgb_resized, dtype=tf.uint8)\n",
    "\n",
    "            # Run pose estimation\n",
    "            pred = model.detect_poses(image_tensor, skeleton='smpl_24')\n",
    "\n",
    "            if 'poses2d' in pred and 'poses3d' in pred:\n",
    "                keypoints_2d = pred['poses2d'].numpy()\n",
    "                keypoints_3d = pred['poses3d'].numpy()\n",
    "                boxes = pred.get('boxes', tf.zeros((keypoints_2d.shape[0], 4))).numpy()  # Get bounding boxes if available\n",
    "\n",
    "                if keypoints_2d.shape[0] == 0:\n",
    "                    print(f\"No pose detected in frame {frame_count}\")\n",
    "                else:\n",
    "                    # Visualize 2D and 3D poses (optional, for debugging)\n",
    "                    # visualize(frame_rgb_resized, boxes, keypoints_3d, keypoints_2d[:, :, :2], skeleton_edges)\n",
    "\n",
    "                    # Assign person IDs (simple assignment based on detection order)\n",
    "                    person_ids = list(range(keypoints_2d.shape[0]))\n",
    "\n",
    "                    # Render 3D skeleton and write to 3D video\n",
    "                    if keypoints_3d.size > 0:\n",
    "                        temp_image_path = render_3d_skeleton(keypoints_3d, person_ids, temp_dir, frame_count, frame_width, frame_height, skeleton_edges)\n",
    "                        if temp_image_path:\n",
    "                            plot_image = cv2.imread(temp_image_path)\n",
    "                            if plot_image is not None:\n",
    "                                plot_image = cv2.resize(plot_image, (frame_width, frame_height))\n",
    "                                out_3d.write(plot_image)\n",
    "                                print(f\"Successfully wrote frame {frame_count} to 3D video\")\n",
    "                            else:\n",
    "                                print(f\"Failed to read 3D plot image for frame {frame_count}\")\n",
    "                    else:\n",
    "                        blank_frame = np.zeros((frame_height, frame_width, 3), dtype=np.uint8)\n",
    "                        out_3d.write(blank_frame)\n",
    "                        print(f\"Wrote blank frame {frame_count} to 3D video (no 3D landmarks)\")\n",
    "\n",
    "                    # Iterate over detected poses (persons)\n",
    "                    for i in range(keypoints_2d.shape[0]):\n",
    "                        if keypoints_2d[i].shape[0] > 0 and keypoints_3d[i].shape[0] > 0:  # Check for valid keypoints\n",
    "                            kpt_2d = keypoints_2d[i]  # Get keypoints for person i\n",
    "                            kpt_3d = keypoints_3d[i]\n",
    "\n",
    "                            # Scale keypoints to original frame size\n",
    "                            kpt_2d[:, 0] *= original_width / 640  # Scale x-coordinates\n",
    "                            kpt_2d[:, 1] *= original_height / 480  # Scale y-coordinates\n",
    "\n",
    "                            # Draw skeleton\n",
    "                            frame = draw_skeleton(frame, kpt_2d, skeleton_edges)\n",
    "\n",
    "                            # Add person label (e.g., \"Person 1\", \"Person 2\", etc.)\n",
    "                            person_label = f\"Person {i + 1}\"  # Person ID starts from 1\n",
    "                            label_position = (int(kpt_2d[0][0]), int(kpt_2d[0][1] - 20))  # Position above the head\n",
    "                            cv2.putText(frame, person_label, label_position, \n",
    "                                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "                            # Store frame-wise keypoints\n",
    "                            for j in range(min(kpt_2d.shape[0], kpt_3d.shape[0])):  # Ensure we only access valid joints\n",
    "                                data_list.append([\n",
    "                                    frame_count, \n",
    "                                    i, \n",
    "                                    j, \n",
    "                                    kpt_2d[j][0], \n",
    "                                    kpt_2d[j][1], \n",
    "                                    kpt_3d[j][0], \n",
    "                                    kpt_3d[j][1], \n",
    "                                    kpt_3d[j][2],  # Include Z coordinate for 3D keypoints\n",
    "                                ])\n",
    "                        else:\n",
    "                            print(f\"Invalid keypoints for frame {frame_count}, person {i}\")\n",
    "\n",
    "        out_2d.write(frame)  # Write frame to 2D video\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    out_2d.release()\n",
    "    out_3d.release()\n",
    "\n",
    "    # Convert collected data to a Pandas DataFrame\n",
    "    df = pd.DataFrame(data_list, columns=['Frame', 'Person', 'Joint', 'X_2D', 'Y_2D', 'X_3D', 'Y_3D', 'Z_3D'])\n",
    "\n",
    "    # Save to Excel file\n",
    "    df.to_excel(output_excel, index=False)\n",
    "    print(f\"Processed video saved as {output_video}\")\n",
    "    print(f\"Pose data saved as {output_excel}\")\n",
    "\n",
    "    # Clean up temporary 3D plot images\n",
    "    import shutil\n",
    "    if os.path.exists(temp_dir):\n",
    "        shutil.rmtree(temp_dir)\n",
    "        print(f\"Cleaned up temporary directory {temp_dir}\")\n",
    "\n",
    "    return df\n",
    "def process_video(video_path, model, skeleton_edges, output_video=\"output_video3d_withlabelschanged.mp4\", frame_interval=10, output_excel=\"2dand3dposedatawithlabelschanged.xlsx\"):\n",
    "    \"\"\"Process video frames with pose estimation, overlay skeletons and person labels, visualize 3D poses, and save coordinates to an Excel file.\"\"\"\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_width = int(cap.get(3))\n",
    "    frame_height = int(cap.get(4))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out_2d = cv2.VideoWriter(output_video, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    # Initialize 3D video output\n",
    "    output_3d_video = \"output_3d_video.mp4\"\n",
    "    out_3d = cv2.VideoWriter(output_3d_video, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    frame_count = 0\n",
    "    data_list = []  # Store pose data\n",
    "\n",
    "    # Create temporary directory for 3D plot images\n",
    "    temp_dir = \"temp_3d_frames\"\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "    # Pre-generate 3D frames and store them\n",
    "    temp_image_paths = []\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if frame_count % frame_interval == 0:\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            original_height, original_width = frame.shape[:2]\n",
    "            frame_rgb_resized = cv2.resize(frame_rgb, (640, 480))\n",
    "            image_tensor = tf.convert_to_tensor(frame_rgb_resized, dtype=tf.uint8)\n",
    "\n",
    "            # Run pose estimation\n",
    "            pred = model.detect_poses(image_tensor, skeleton='smpl_24')\n",
    "\n",
    "            if 'poses2d' in pred and 'poses3d' in pred:\n",
    "                keypoints_2d = pred['poses2d'].numpy()\n",
    "                keypoints_3d = pred['poses3d'].numpy()\n",
    "                boxes = pred.get('boxes', tf.zeros((keypoints_2d.shape[0], 4))).numpy()  # Get bounding boxes if available\n",
    "\n",
    "                if keypoints_2d.shape[0] == 0:\n",
    "                    print(f\"No pose detected in frame {frame_count}\")\n",
    "                else:\n",
    "                    # Assign person IDs (simple assignment based on detection order)\n",
    "                    person_ids = list(range(keypoints_2d.shape[0]))\n",
    "\n",
    "                    # Render 3D skeleton and store the image path\n",
    "                    if keypoints_3d.size > 0:\n",
    "                        temp_image_path = render_3d_skeleton(keypoints_3d, person_ids, temp_dir, frame_count, frame_width, frame_height, skeleton_edges)\n",
    "                        if temp_image_path:\n",
    "                            temp_image_paths.append(temp_image_path)\n",
    "                            print(f\"Generated 3D frame for frame {frame_count}\")\n",
    "                    else:\n",
    "                        blank_frame = np.zeros((frame_height, frame_width, 3), dtype=np.uint8)\n",
    "                        temp_image_path = os.path.join(temp_dir, f\"3d_frame_{frame_count}.png\")\n",
    "                        cv2.imwrite(temp_image_path, blank_frame)\n",
    "                        temp_image_paths.append(temp_image_path)\n",
    "                        print(f\"Generated blank 3D frame for frame {frame_count}\")\n",
    "\n",
    "                    # Iterate over detected poses (persons)\n",
    "                    for i in range(keypoints_2d.shape[0]):\n",
    "                        if keypoints_2d[i].shape[0] > 0 and keypoints_3d[i].shape[0] > 0:  # Check for valid keypoints\n",
    "                            kpt_2d = keypoints_2d[i]  # Get keypoints for person i\n",
    "                            kpt_3d = keypoints_3d[i]\n",
    "\n",
    "                            # Scale keypoints to original frame size\n",
    "                            kpt_2d[:, 0] *= original_width / 640  # Scale x-coordinates\n",
    "                            kpt_2d[:, 1] *= original_height / 480  # Scale y-coordinates\n",
    "\n",
    "                            # Draw skeleton\n",
    "                            frame = draw_skeleton(frame, kpt_2d, skeleton_edges)\n",
    "\n",
    "                            # Add person label (e.g., \"Person 1\", \"Person 2\", etc.)\n",
    "                            person_label = f\"Person {i + 1}\"  # Person ID starts from 1\n",
    "                            label_position = (int(kpt_2d[0][0]), int(kpt_2d[0][1] - 20))  # Position above the head\n",
    "                            cv2.putText(frame, person_label, label_position, \n",
    "                                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "                            # Store frame-wise keypoints\n",
    "                            for j in range(min(kpt_2d.shape[0], kpt_3d.shape[0])):  # Ensure we only access valid joints\n",
    "                                data_list.append([\n",
    "                                    frame_count, \n",
    "                                    i, \n",
    "                                    j, \n",
    "                                    kpt_2d[j][0], \n",
    "                                    kpt_2d[j][1], \n",
    "                                    kpt_3d[j][0], \n",
    "                                    kpt_3d[j][1], \n",
    "                                    kpt_3d[j][2],  # Include Z coordinate for 3D keypoints\n",
    "                                ])\n",
    "                        else:\n",
    "                            print(f\"Invalid keypoints for frame {frame_count}, person {i}\")\n",
    "\n",
    "        out_2d.write(frame)  # Write frame to 2D video\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    out_2d.release()\n",
    "\n",
    "    # Write 3D video with frame duplication to match original duration\n",
    "    for i in range(total_frames):\n",
    "        frame_idx = i // (frame_interval if frame_interval > 0 else 1)  # Map to nearest rendered frame\n",
    "        if frame_idx < len(temp_image_paths):\n",
    "            plot_image = cv2.imread(temp_image_paths[frame_idx])\n",
    "            if plot_image is not None:\n",
    "                plot_image = cv2.resize(plot_image, (frame_width, frame_height))\n",
    "                out_3d.write(plot_image)\n",
    "                print(f\"Wrote frame {i} to 3D video (from {temp_image_paths[frame_idx]})\")\n",
    "            else:\n",
    "                blank_frame = np.zeros((frame_height, frame_width, 3), dtype=np.uint8)\n",
    "                out_3d.write(blank_frame)\n",
    "                print(f\"Wrote blank frame {i} to 3D video (image load failed)\")\n",
    "        else:\n",
    "            blank_frame = np.zeros((frame_height, frame_width, 3), dtype=np.uint8)\n",
    "            out_3d.write(blank_frame)\n",
    "            print(f\"Wrote blank frame {i} to 3D video (no corresponding 3D frame)\")\n",
    "\n",
    "    out_3d.release()\n",
    "\n",
    "    # Convert collected data to a Pandas DataFrame\n",
    "    df = pd.DataFrame(data_list, columns=['Frame', 'Person', 'Joint', 'X_2D', 'Y_2D', 'X_3D', 'Y_3D', 'Z_3D'])\n",
    "\n",
    "    # Save to Excel file\n",
    "    df.to_excel(output_excel, index=False)\n",
    "    print(f\"Processed video saved as {output_video}\")\n",
    "    print(f\"Pose data saved as {output_excel}\")\n",
    "\n",
    "    # Clean up temporary 3D plot images\n",
    "    import shutil\n",
    "    if os.path.exists(temp_dir):\n",
    "        shutil.rmtree(temp_dir)\n",
    "        print(f\"Cleaned up temporary directory {temp_dir}\")\n",
    "\n",
    "    return df\n",
    "#def process_video(video_path, model, skeleton_edges, output_video=\"output_video3d_withlabelschanged.mp4\", output_excel=\"2dand3dposedatawithlabelschanged.xlsx\"):\n",
    "    \"\"\"Process video frames with pose estimation, overlay skeletons and person labels, visualize 3D poses, and save coordinates to an Excel file.\"\"\"\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_width = int(cap.get(3))\n",
    "    frame_height = int(cap.get(4))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out_2d = cv2.VideoWriter(output_video, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    # Initialize 3D video output\n",
    "    output_3d_video = \"output_3d_video.mp4\"\n",
    "    out_3d = cv2.VideoWriter(output_3d_video, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    frame_count = 0\n",
    "    data_list = []  # Store pose data\n",
    "\n",
    "    # Create temporary directory for 3D plot images\n",
    "    temp_dir = \"temp_3d_frames\"\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        original_height, original_width = frame.shape[:2]\n",
    "        frame_rgb_resized = cv2.resize(frame_rgb, (640, 480))\n",
    "        image_tensor = tf.convert_to_tensor(frame_rgb_resized, dtype=tf.uint8)\n",
    "\n",
    "        # Run pose estimation for every frame\n",
    "        pred = model.detect_poses(image_tensor, skeleton='smpl_24')\n",
    "\n",
    "        if 'poses2d' in pred and 'poses3d' in pred:\n",
    "            keypoints_2d = pred['poses2d'].numpy()\n",
    "            keypoints_3d = pred['poses3d'].numpy()\n",
    "            boxes = pred.get('boxes', tf.zeros((keypoints_2d.shape[0], 4))).numpy()  # Get bounding boxes if available\n",
    "\n",
    "            if keypoints_2d.shape[0] == 0:\n",
    "                print(f\"No pose detected in frame {frame_count}\")\n",
    "            else:\n",
    "                # Assign person IDs (simple assignment based on detection order)\n",
    "                person_ids = list(range(keypoints_2d.shape[0]))\n",
    "\n",
    "                # Render 3D skeleton and write to 3D video\n",
    "                if keypoints_3d.size > 0:\n",
    "                    temp_image_path = render_3d_skeleton(keypoints_3d, person_ids, temp_dir, frame_count, frame_width, frame_height, skeleton_edges)\n",
    "                    if temp_image_path:\n",
    "                        plot_image = cv2.imread(temp_image_path)\n",
    "                        if plot_image is not None:\n",
    "                            plot_image = cv2.resize(plot_image, (frame_width, frame_height))\n",
    "                            out_3d.write(plot_image)\n",
    "                            print(f\"Successfully wrote frame {frame_count} to 3D video\")\n",
    "                        else:\n",
    "                            print(f\"Failed to read 3D plot image for frame {frame_count}\")\n",
    "                    else:\n",
    "                        blank_frame = np.zeros((frame_height, frame_width, 3), dtype=np.uint8)\n",
    "                        out_3d.write(blank_frame)\n",
    "                        print(f\"Wrote blank frame {frame_count} to 3D video (render failed)\")\n",
    "                else:\n",
    "                    blank_frame = np.zeros((frame_height, frame_width, 3), dtype=np.uint8)\n",
    "                    out_3d.write(blank_frame)\n",
    "                    print(f\"Wrote blank frame {frame_count} to 3D video (no 3D landmarks)\")\n",
    "\n",
    "                # Iterate over detected poses (persons)\n",
    "                for i in range(keypoints_2d.shape[0]):\n",
    "                    if keypoints_2d[i].shape[0] > 0 and keypoints_3d[i].shape[0] > 0:  # Check for valid keypoints\n",
    "                        kpt_2d = keypoints_2d[i]  # Get keypoints for person i\n",
    "                        kpt_3d = keypoints_3d[i]\n",
    "\n",
    "                        # Scale keypoints to original frame size\n",
    "                        kpt_2d[:, 0] *= original_width / 640  # Scale x-coordinates\n",
    "                        kpt_2d[:, 1] *= original_height / 480  # Scale y-coordinates\n",
    "\n",
    "                        # Draw skeleton\n",
    "                        frame = draw_skeleton(frame, kpt_2d, skeleton_edges)\n",
    "\n",
    "                        # Add person label (e.g., \"Person 1\", \"Person 2\", etc.)\n",
    "                        person_label = f\"Person {i + 1}\"  # Person ID starts from 1\n",
    "                        label_position = (int(kpt_2d[0][0]), int(kpt_2d[0][1] - 20))  # Position above the head\n",
    "                        cv2.putText(frame, person_label, label_position, \n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "                        # Store frame-wise keypoints\n",
    "                        for j in range(min(kpt_2d.shape[0], kpt_3d.shape[0])):  # Ensure we only access valid joints\n",
    "                            data_list.append([\n",
    "                                frame_count, \n",
    "                                i, \n",
    "                                j, \n",
    "                                kpt_2d[j][0], \n",
    "                                kpt_2d[j][1], \n",
    "                                kpt_3d[j][0], \n",
    "                                kpt_3d[j][1], \n",
    "                                kpt_3d[j][2],  # Include Z coordinate for 3D keypoints\n",
    "                            ])\n",
    "                    else:\n",
    "                        print(f\"Invalid keypoints for frame {frame_count}, person {i}\")\n",
    "\n",
    "        out_2d.write(frame)  # Write frame to 2D video\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    out_2d.release()\n",
    "    out_3d.release()\n",
    "\n",
    "    # Convert collected data to a Pandas DataFrame\n",
    "    df = pd.DataFrame(data_list, columns=['Frame', 'Person', 'Joint', 'X_2D', 'Y_2D', 'X_3D', 'Y_3D', 'Z_3D'])\n",
    "\n",
    "    # Save to Excel file\n",
    "    df.to_excel(output_excel, index=False)\n",
    "    print(f\"Processed video saved as {output_video}\")\n",
    "    print(f\"Pose data saved as {output_excel}\")\n",
    "\n",
    "    # Clean up temporary 3D plot images\n",
    "    import shutil\n",
    "    if os.path.exists(temp_dir):\n",
    "        shutil.rmtree(temp_dir)\n",
    "        print(f\"Cleaned up temporary directory {temp_dir}\")\n",
    "\n",
    "    return df\n",
    "#def process_video(video_path, model, skeleton_edges, output_video=\"output_video3d_withlabelschanged.mp4\", output_excel=\"2dand3dposedatawithlabelschanged.xlsx\"):\n",
    "    \"\"\"Process video frames with pose estimation, overlay skeletons and person labels, visualize 3D poses, and save coordinates to an Excel file.\"\"\"\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_width = int(cap.get(3))\n",
    "    frame_height = int(cap.get(4))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out_2d = cv2.VideoWriter(output_video, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    # Initialize 3D video output\n",
    "    output_3d_video = \"output_3d_video.mp4\"\n",
    "    out_3d = cv2.VideoWriter(output_3d_video, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    frame_count = 0\n",
    "    data_list = []  # Store pose data\n",
    "\n",
    "    # Create temporary directory for 3D plot images\n",
    "    temp_dir = \"temp_3d_frames\"\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        original_height, original_width = frame.shape[:2]\n",
    "        frame_rgb_resized = cv2.resize(frame_rgb, (640, 480))\n",
    "        image_tensor = tf.convert_to_tensor(frame_rgb_resized, dtype=tf.uint8)\n",
    "\n",
    "        # Run pose estimation for every frame\n",
    "        pred = model.detect_poses(image_tensor, skeleton='smpl_24')\n",
    "\n",
    "        if 'poses2d' in pred and 'poses3d' in pred:\n",
    "            keypoints_2d = pred['poses2d'].numpy()\n",
    "            keypoints_3d = pred['poses3d'].numpy()\n",
    "            boxes = pred.get('boxes', tf.zeros((keypoints_2d.shape[0], 4))).numpy()  # Get bounding boxes if available\n",
    "\n",
    "            if keypoints_2d.shape[0] == 0:\n",
    "                print(f\"No pose detected in frame {frame_count}\")\n",
    "            else:\n",
    "                # Assign person IDs (simple assignment based on detection order)\n",
    "                person_ids = list(range(keypoints_2d.shape[0]))\n",
    "\n",
    "                # Render 3D skeleton and write to 3D video\n",
    "                if keypoints_3d.size > 0:\n",
    "                    temp_image_path = render_3d_skeleton(keypoints_3d, person_ids, temp_dir, frame_count, frame_width, frame_height, skeleton_edges)\n",
    "                    if temp_image_path:\n",
    "                        plot_image = cv2.imread(temp_image_path)\n",
    "                        if plot_image is not None:\n",
    "                            # Remove resizing to preserve original plot dimensions\n",
    "                            out_3d.write(plot_image)\n",
    "                            print(f\"Successfully wrote frame {frame_count} to 3D video\")\n",
    "                        else:\n",
    "                            print(f\"Failed to read 3D plot image for frame {frame_count}\")\n",
    "                    else:\n",
    "                        blank_frame = np.zeros((frame_height, frame_width, 3), dtype=np.uint8)\n",
    "                        out_3d.write(blank_frame)\n",
    "                        print(f\"Wrote blank frame {frame_count} to 3D video (render failed)\")\n",
    "                else:\n",
    "                    blank_frame = np.zeros((frame_height, frame_width, 3), dtype=np.uint8)\n",
    "                    out_3d.write(blank_frame)\n",
    "                    print(f\"Wrote blank frame {frame_count} to 3D video (no 3D landmarks)\")\n",
    "\n",
    "                # Iterate over detected poses (persons)\n",
    "                for i in range(keypoints_2d.shape[0]):\n",
    "                    if keypoints_2d[i].shape[0] > 0 and keypoints_3d[i].shape[0] > 0:  # Check for valid keypoints\n",
    "                        kpt_2d = keypoints_2d[i]  # Get keypoints for person i\n",
    "                        kpt_3d = keypoints_3d[i]\n",
    "\n",
    "                        # Scale keypoints to original frame size\n",
    "                        kpt_2d[:, 0] *= original_width / 640  # Scale x-coordinates\n",
    "                        kpt_2d[:, 1] *= original_height / 480  # Scale y-coordinates\n",
    "\n",
    "                        # Draw skeleton\n",
    "                        frame = draw_skeleton(frame, kpt_2d, skeleton_edges)\n",
    "\n",
    "                        # Add person label (e.g., \"Person 1\", \"Person 2\", etc.)\n",
    "                        person_label = f\"Person {i + 1}\"  # Person ID starts from 1\n",
    "                        label_position = (int(kpt_2d[0][0]), int(kpt_2d[0][1] - 20))  # Position above the head\n",
    "                        cv2.putText(frame, person_label, label_position, \n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "                        # Store frame-wise keypoints\n",
    "                        for j in range(min(kpt_2d.shape[0], kpt_3d.shape[0])):  # Ensure we only access valid joints\n",
    "                            data_list.append([\n",
    "                                frame_count, \n",
    "                                i, \n",
    "                                j, \n",
    "                                kpt_2d[j][0], \n",
    "                                kpt_2d[j][1], \n",
    "                                kpt_3d[j][0], \n",
    "                                kpt_3d[j][1], \n",
    "                                kpt_3d[j][2],  # Include Z coordinate for 3D keypoints\n",
    "                            ])\n",
    "                    else:\n",
    "                        print(f\"Invalid keypoints for frame {frame_count}, person {i}\")\n",
    "\n",
    "        out_2d.write(frame)  # Write frame to 2D video\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    out_2d.release()\n",
    "    out_3d.release()\n",
    "\n",
    "    # Convert collected data to a Pandas DataFrame\n",
    "    df = pd.DataFrame(data_list, columns=['Frame', 'Person', 'Joint', 'X_2D', 'Y_2D', 'X_3D', 'Y_3D', 'Z_3D'])\n",
    "\n",
    "    # Save to Excel file\n",
    "    df.to_excel(output_excel, index=False)\n",
    "    print(f\"Processed video saved as {output_video}\")\n",
    "    print(f\"Pose data saved as {output_excel}\")\n",
    "\n",
    "    # Clean up temporary 3D plot images\n",
    "    import shutil\n",
    "    if os.path.exists(temp_dir):\n",
    "        shutil.rmtree(temp_dir)\n",
    "        print(f\"Cleaned up temporary directory {temp_dir}\")\n",
    "\n",
    "    return df\n",
    "def calculate_gait_metrics_2d(df, person_id, fps, skeleton_edges, joint_names):\n",
    "    \"\"\"Calculate 2D gait metrics for a specific person.\"\"\"\n",
    "    joint_map = {\n",
    "        'pelvis': 0, 'left_hip': 1, 'right_hip': 2, 'spine1': 3,\n",
    "        'left_knee': 4, 'right_knee': 5, 'spine2': 6,\n",
    "        'left_ankle': 7, 'right_ankle': 8\n",
    "    }\n",
    "\n",
    "    df_person = df[df['Person'] == person_id]\n",
    "    if df_person.empty:\n",
    "        return None  # No data for this person\n",
    "\n",
    "    left_ankle_2d = df_person[df_person['Joint'] == joint_map['left_ankle']][['Frame', 'X_2D', 'Y_2D']].values\n",
    "    right_ankle_2d = df_person[df_person['Joint'] == joint_map['right_ankle']][['Frame', 'X_2D', 'Y_2D']].values\n",
    "\n",
    "    # Detect heel strikes in 2D (lowest Y_2D position)\n",
    "    left_strikes_2d = []\n",
    "    right_strikes_2d = []\n",
    "    for i in range(1, len(left_ankle_2d) - 1):\n",
    "        if left_ankle_2d[i][2] < left_ankle_2d[i-1][2] and left_ankle_2d[i][2] < left_ankle_2d[i+1][2]:\n",
    "            left_strikes_2d.append(left_ankle_2d[i])\n",
    "        if right_ankle_2d[i][2] < right_ankle_2d[i-1][2] and right_ankle_2d[i][2] < right_ankle_2d[i+1][2]:\n",
    "            right_strikes_2d.append(right_ankle_2d[i])\n",
    "\n",
    "    left_strikes_2d = np.array(left_strikes_2d)\n",
    "    right_strikes_2d = np.array(right_strikes_2d)\n",
    "\n",
    "    # 1. Stride Time (2D)\n",
    "    stride_time_left_2d = np.mean(np.diff(left_strikes_2d[:, 0]) / fps) if len(left_strikes_2d) > 1 else np.nan\n",
    "    stride_time_right_2d = np.mean(np.diff(right_strikes_2d[:, 0]) / fps) if len(right_strikes_2d) > 1 else np.nan\n",
    "\n",
    "    # 2. Stride Length (2D, in pixels)\n",
    "    stride_length_left_2d = np.mean([distance.euclidean(left_strikes_2d[i][1:3], left_strikes_2d[i+1][1:3]) \n",
    "                                    for i in range(len(left_strikes_2d)-1)]) if len(left_strikes_2d) > 1 else np.nan\n",
    "    stride_length_right_2d = np.mean([distance.euclidean(right_strikes_2d[i][1:3], right_strikes_2d[i+1][1:3]) \n",
    "                                     for i in range(len(right_strikes_2d)-1)]) if len(right_strikes_2d) > 1 else np.nan\n",
    "\n",
    "    # 3. Cadence (2D)\n",
    "    total_steps_2d = len(left_strikes_2d) + len(right_strikes_2d)\n",
    "    total_time_2d = (df_person['Frame'].max() - df_person['Frame'].min()) / fps / 60 if not df_person['Frame'].empty else 0\n",
    "    cadence_2d = total_steps_2d / total_time_2d if total_time_2d > 0 else np.nan\n",
    "\n",
    "    # 4. Gait Speed (2D, pixels per second)\n",
    "    gait_speed_left_2d = stride_length_left_2d / stride_time_left_2d if stride_time_left_2d > 0 else np.nan\n",
    "    gait_speed_right_2d = stride_length_right_2d / stride_time_right_2d if stride_time_right_2d > 0 else np.nan\n",
    "\n",
    "    # 5. Knee and Hip Angles (2D)\n",
    "    angles_2d = {'left_knee': [], 'right_knee': [], 'left_hip': [], 'right_hip': []}\n",
    "    for frame in df_person['Frame'].unique():\n",
    "        frame_data = df_person[df_person['Frame'] == frame]\n",
    "        \n",
    "        def get_coords_2d(joint):\n",
    "            row = frame_data[frame_data['Joint'] == joint_map[joint]]\n",
    "            return row[['X_2D', 'Y_2D']].values[0] if not row.empty else np.array([np.nan, np.nan])\n",
    "\n",
    "        left_hip_2d = get_coords_2d('left_hip')\n",
    "        left_knee_2d = get_coords_2d('left_knee')\n",
    "        left_ankle_2d = get_coords_2d('left_ankle')\n",
    "        right_hip_2d = get_coords_2d('right_hip')\n",
    "        right_knee_2d = get_coords_2d('right_knee')\n",
    "        right_ankle_2d = get_coords_2d('right_ankle')\n",
    "        spine_2d = get_coords_2d('spine1')\n",
    "\n",
    "        def angle_between_2d(v1, v2):\n",
    "            if np.any(np.isnan(v1)) or np.any(np.isnan(v2)):\n",
    "                return np.nan\n",
    "            cos_theta = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "            return np.degrees(np.arccos(np.clip(cos_theta, -1.0, 1.0)))\n",
    "\n",
    "        angles_2d['left_knee'].append(angle_between_2d(left_hip_2d - left_knee_2d, left_ankle_2d - left_knee_2d))\n",
    "        angles_2d['right_knee'].append(angle_between_2d(right_hip_2d - right_knee_2d, right_ankle_2d - right_knee_2d))\n",
    "        angles_2d['left_hip'].append(angle_between_2d(spine_2d - left_hip_2d, left_knee_2d - left_hip_2d))\n",
    "        angles_2d['right_hip'].append(angle_between_2d(spine_2d - right_hip_2d, right_knee_2d - right_hip_2d))\n",
    "\n",
    "    avg_angles_2d = {k: np.nanmean(v) for k, v in angles_2d.items()}\n",
    "\n",
    "    return {\n",
    "        'Person': person_id,\n",
    "        'Stride Time Left (s)': stride_time_left_2d,\n",
    "        'Stride Time Right (s)': stride_time_right_2d,\n",
    "        'Stride Length Left (mm)': stride_length_left_2d,\n",
    "        'Stride Length Right (mm)': stride_length_right_2d,\n",
    "        'Cadence (steps/min)': cadence_2d,\n",
    "        'Gait Speed Left (mm/s)': gait_speed_left_2d,\n",
    "        'Gait Speed Right (mm/s)': gait_speed_right_2d,\n",
    "        'Avg Left Knee Angle (deg)': avg_angles_2d['left_knee'],\n",
    "        'Avg Right Knee Angle (deg)': avg_angles_2d['right_knee'],\n",
    "        'Avg Left Hip Angle (deg)': avg_angles_2d['left_hip'],\n",
    "        'Avg Right Hip Angle (deg)': avg_angles_2d['right_hip']\n",
    "    }\n",
    "def calculate_gait_metrics_3d(df, person_id, fps, skeleton_edges, joint_names):\n",
    "    \"\"\"Calculate 3D gait metrics for a specific person.\"\"\"\n",
    "    joint_map = {\n",
    "        'pelvis': 0, 'left_hip': 1, 'right_hip': 2, 'spine1': 3,\n",
    "        'left_knee': 4, 'right_knee': 5, 'spine2': 6,\n",
    "        'left_ankle': 7, 'right_ankle': 8\n",
    "    }\n",
    "\n",
    "    df_person = df[df['Person'] == person_id]\n",
    "    if df_person.empty:\n",
    "        return None  # No data for this person\n",
    "\n",
    "    left_ankle_3d = df_person[df_person['Joint'] == joint_map['left_ankle']][['Frame', 'X_3D', 'Y_3D', 'Z_3D']].values\n",
    "    right_ankle_3d = df_person[df_person['Joint'] == joint_map['right_ankle']][['Frame', 'X_3D', 'Y_3D', 'Z_3D']].values\n",
    "\n",
    "    # Detect heel strikes in 3D (lowest Z_3D position)\n",
    "    left_strikes_3d = []\n",
    "    right_strikes_3d = []\n",
    "    for i in range(1, len(left_ankle_3d) - 1):\n",
    "        if left_ankle_3d[i][3] < left_ankle_3d[i-1][3] and left_ankle_3d[i][3] < left_ankle_3d[i+1][3]:\n",
    "            left_strikes_3d.append(left_ankle_3d[i])\n",
    "        if right_ankle_3d[i][3] < right_ankle_3d[i-1][3] and right_ankle_3d[i][3] < right_ankle_3d[i+1][3]:\n",
    "            right_strikes_3d.append(right_ankle_3d[i])\n",
    "\n",
    "    left_strikes_3d = np.array(left_strikes_3d)\n",
    "    right_strikes_3d = np.array(right_strikes_3d)\n",
    "\n",
    "    # 1. Stride Time (3D)\n",
    "    stride_time_left_3d = np.mean(np.diff(left_strikes_3d[:, 0]) / fps) if len(left_strikes_3d) > 1 else np.nan\n",
    "    stride_time_right_3d = np.mean(np.diff(right_strikes_3d[:, 0]) / fps) if len(right_strikes_3d) > 1 else np.nan\n",
    "\n",
    "    # 2. Stride Length (3D, assumed meters)\n",
    "    stride_length_left_3d = np.mean([distance.euclidean(left_strikes_3d[i][1:4], left_strikes_3d[i+1][1:4]) \n",
    "                                    for i in range(len(left_strikes_3d)-1)]) if len(left_strikes_3d) > 1 else np.nan\n",
    "    stride_length_right_3d = np.mean([distance.euclidean(right_strikes_3d[i][1:4], right_strikes_3d[i+1][1:4]) \n",
    "                                     for i in range(len(right_strikes_3d)-1)]) if len(right_strikes_3d) > 1 else np.nan\n",
    "\n",
    "    # 3. Cadence (3D)\n",
    "    total_steps_3d = len(left_strikes_3d) + len(right_strikes_3d)\n",
    "    total_time_3d = (df_person['Frame'].max() - df_person['Frame'].min()) / fps / 60 if not df_person['Frame'].empty else 0\n",
    "    cadence_3d = total_steps_3d / total_time_3d if total_time_3d > 0 else np.nan\n",
    "\n",
    "    # 4. Gait Speed (3D, assumed meters per second)\n",
    "    gait_speed_left_3d = stride_length_left_3d / stride_time_left_3d if stride_time_left_3d > 0 else np.nan\n",
    "    gait_speed_right_3d = stride_length_right_3d / stride_time_right_3d if stride_time_right_3d > 0 else np.nan\n",
    "\n",
    "    # 5. Knee and Hip Angles (3D)\n",
    "    angles_3d = {'left_knee': [], 'right_knee': [], 'left_hip': [], 'right_hip': []}\n",
    "    for frame in df_person['Frame'].unique():\n",
    "        frame_data = df_person[df_person['Frame'] == frame]\n",
    "        \n",
    "        def get_coords_3d(joint):\n",
    "            row = frame_data[frame_data['Joint'] == joint_map[joint]]\n",
    "            return row[['X_3D', 'Y_3D', 'Z_3D']].values[0] if not row.empty else np.array([np.nan, np.nan, np.nan])\n",
    "\n",
    "        left_hip_3d = get_coords_3d('left_hip')\n",
    "        left_knee_3d = get_coords_3d('left_knee')\n",
    "        left_ankle_3d = get_coords_3d('left_ankle')\n",
    "        right_hip_3d = get_coords_3d('right_hip')\n",
    "        right_knee_3d = get_coords_3d('right_knee')\n",
    "        right_ankle_3d = get_coords_3d('right_ankle')\n",
    "        spine_3d = get_coords_3d('spine1')\n",
    "\n",
    "        def angle_between_3d(v1, v2):\n",
    "            if np.any(np.isnan(v1)) or np.any(np.isnan(v2)):\n",
    "                return np.nan\n",
    "            cos_theta = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "            return np.degrees(np.arccos(np.clip(cos_theta, -1.0, 1.0)))\n",
    "\n",
    "        angles_3d['left_knee'].append(angle_between_3d(left_hip_3d - left_knee_3d, left_ankle_3d - left_knee_3d))\n",
    "        angles_3d['right_knee'].append(angle_between_3d(right_hip_3d - right_knee_3d, right_ankle_3d - right_knee_3d))\n",
    "        angles_3d['left_hip'].append(angle_between_3d(spine_3d - left_hip_3d, left_knee_3d - left_hip_3d))\n",
    "        angles_3d['right_hip'].append(angle_between_3d(spine_3d - right_hip_3d, right_knee_3d - right_hip_3d))\n",
    "\n",
    "    avg_angles_3d = {k: np.nanmean(v) for k, v in angles_3d.items()}\n",
    "\n",
    "    return {\n",
    "        'Person': person_id,\n",
    "        'Stride Time Left (s)': stride_time_left_3d,\n",
    "        'Stride Time Right (s)': stride_time_right_3d,\n",
    "        'Stride Length Left (mm)': stride_length_left_3d,\n",
    "        'Stride Length Right (mm)': stride_length_right_3d,\n",
    "        'Cadence (steps/min)': cadence_3d,\n",
    "        'Gait Speed Left (mm/s)': gait_speed_left_3d,\n",
    "        'Gait Speed Right (mm/s)': gait_speed_right_3d,\n",
    "        'Avg Left Knee Angle (deg)': avg_angles_3d['left_knee'],\n",
    "        'Avg Right Knee Angle (deg)': avg_angles_3d['right_knee'],\n",
    "        'Avg Left Hip Angle (deg)': avg_angles_3d['left_hip'],\n",
    "        'Avg Right Hip Angle (deg)': avg_angles_3d['right_hip']\n",
    "    }\n",
    "#def render_3d_skeleton(poses3d, person_ids, temp_dir, frame_idx, frame_width, frame_height, skeleton_edges):\n",
    "    \"\"\"Render the 3D skeleton as an image for a single frame.\"\"\"\n",
    "    try:\n",
    "        fig = plt.figure(figsize=(frame_width / 100, frame_height / 100), dpi=100)  # Match video dimensions\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.view_init(5, -85)\n",
    "\n",
    "        # Collect all coordinates to determine dynamic axis limits\n",
    "        all_x_3d, all_y_3d, all_z_3d = [], [], []\n",
    "        for pose3d in poses3d:\n",
    "            all_x_3d.extend(pose3d[:, 0])\n",
    "            all_y_3d.extend(pose3d[:, 1])\n",
    "            all_z_3d.extend(pose3d[:, 2])\n",
    "\n",
    "        # Set dynamic axis limits with padding\n",
    "        if all_x_3d:\n",
    "            x_min, x_max = min(all_x_3d), max(all_x_3d)\n",
    "            y_min, y_max = min(all_y_3d), max(all_y_3d)\n",
    "            z_min, z_max = min(all_z_3d), max(all_z_3d)\n",
    "            padding = 0.5  # Add padding in meters (adjust based on Metrabs scale)\n",
    "            ax.set_xlim(x_min - padding, x_max + padding)\n",
    "            ax.set_ylim(y_min - padding, y_max + padding)\n",
    "            ax.set_zlim(z_min - padding, z_max + padding)\n",
    "        else:\n",
    "            ax.set_xlim(-1.5, 1.5)\n",
    "            ax.set_ylim(-1.5, 1.5)\n",
    "            ax.set_zlim(0, 3.0)\n",
    "\n",
    "        ax.set_xlabel('X (m)')\n",
    "        ax.set_ylabel('Y (m)')\n",
    "        ax.set_zlabel('Z (m)')\n",
    "\n",
    "        # Plot 3D skeleton for each person\n",
    "        for pose3d, person_id in zip(poses3d, person_ids):\n",
    "            if person_id == -1:\n",
    "                continue\n",
    "            # Plot skeleton connections\n",
    "            for i_start, i_end in skeleton_edges:\n",
    "                ax.plot(*zip(pose3d[i_start], pose3d[i_end]), 'g-', marker='o', markersize=2)\n",
    "            ax.scatter(*pose3d.T, c='g', s=2)\n",
    "            # Add person label above the head (first joint, e.g., pelvis)\n",
    "            ax.text(pose3d[0, 0], pose3d[0, 1], pose3d[0, 2] + 0.2, \n",
    "                    f\"Person {person_id + 1}\", color='red')\n",
    "\n",
    "        fig.tight_layout()\n",
    "        # Save the plot as an image\n",
    "        temp_image_path = os.path.join(temp_dir, f\"3d_frame_{frame_idx}.png\")\n",
    "        plt.savefig(temp_image_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        return temp_image_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error rendering 3D skeleton for frame {frame_idx}: {str(e)}\")\n",
    "        return None\n",
    "def render_3d_skeleton(poses3d, person_ids, temp_dir, frame_idx, frame_width, frame_height, skeleton_edges):\n",
    "    \"\"\"Render the 3D skeleton as an image for a single frame.\"\"\"\n",
    "    try:\n",
    "        fig = plt.figure(figsize=(frame_width / 100, frame_height / 100), dpi=100)  # Match video dimensions\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "        # Adjust view angle for a more natural perspective (front-side view)\n",
    "        ax.view_init(elev=20, azim=45)  # elev=20 for slight top-down, azim=45 for front-side view\n",
    "\n",
    "        # Collect all coordinates to determine dynamic axis limits\n",
    "        all_x_3d, all_y_3d, all_z_3d = [], [], []\n",
    "        for pose3d in poses3d:\n",
    "            # Apply axis transformation: swap Y and Z, negate Z to match standard orientation\n",
    "            pose3d_adjusted = pose3d.copy()\n",
    "            pose3d_adjusted[:, 1], pose3d_adjusted[:, 2] = pose3d[:, 2], -pose3d[:, 1]\n",
    "            all_x_3d.extend(pose3d_adjusted[:, 0])\n",
    "            all_y_3d.extend(pose3d_adjusted[:, 1])\n",
    "            all_z_3d.extend(pose3d_adjusted[:, 2])\n",
    "\n",
    "        # Set dynamic axis limits with padding\n",
    "        if all_x_3d:\n",
    "            x_min, x_max = min(all_x_3d), max(all_x_3d)\n",
    "            y_min, y_max = min(all_y_3d), max(all_y_3d)\n",
    "            z_min, z_max = min(all_z_3d), max(all_z_3d)\n",
    "            padding = 0.5  # Add padding in meters\n",
    "            ax.set_xlim(x_min - padding, x_max + padding)\n",
    "            ax.set_ylim(y_min - padding, y_max + padding)\n",
    "            ax.set_zlim(z_min - padding, z_max + padding)\n",
    "        else:\n",
    "            ax.set_xlim(-1.5, 1.5)\n",
    "            ax.set_ylim(-1.5, 1.5)\n",
    "            ax.set_zlim(0, 3.0)\n",
    "\n",
    "        ax.set_xlabel('X (m)')\n",
    "        ax.set_ylabel('Y (m)')\n",
    "        ax.set_zlabel('Z (m)')\n",
    "\n",
    "        # Plot 3D skeleton for each person\n",
    "        for pose3d, person_id in zip(poses3d, person_ids):\n",
    "            if person_id == -1:\n",
    "                continue\n",
    "            # Apply axis transformation for plotting\n",
    "            pose3d_adjusted = pose3d.copy()\n",
    "            pose3d_adjusted[:, 1], pose3d_adjusted[:, 2] = pose3d[:, 2], -pose3d[:, 1]\n",
    "            # Plot skeleton connections\n",
    "            for i_start, i_end in skeleton_edges:\n",
    "                ax.plot(*zip(pose3d_adjusted[i_start], pose3d_adjusted[i_end]), 'g-', marker='o', markersize=2)\n",
    "            ax.scatter(*pose3d_adjusted.T, c='g', s=2)\n",
    "            # Add person label above the head (first joint, e.g., pelvis)\n",
    "            ax.text(pose3d_adjusted[0, 0], pose3d_adjusted[0, 1], pose3d_adjusted[0, 2] + 0.2, \n",
    "                    f\"Person {person_id + 1}\", color='red')\n",
    "\n",
    "        fig.tight_layout()\n",
    "        # Save the plot as an image\n",
    "        temp_image_path = os.path.join(temp_dir, f\"3d_frame_{frame_idx}.png\")\n",
    "        plt.savefig(temp_image_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        return temp_image_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error rendering 3D skeleton for frame {frame_idx}: {str(e)}\")\n",
    "        return None  \n",
    "#def render_3d_skeleton(poses3d, person_ids, temp_dir, frame_idx, frame_width, frame_height, skeleton_edges):\n",
    "    \"\"\"Render the 3D skeleton as an image for a single frame.\"\"\"\n",
    "    try:\n",
    "        # Use a fixed figsize to ensure consistent 3D aspect ratio (adjust as needed)\n",
    "        fig = plt.figure(figsize=(10, 10), dpi=100)  # Square figure for better 3D proportion\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "        # Set view angle for a front-facing perspective\n",
    "        ax.view_init(elev=15, azim=180)  # elev=15 for slight top-down, azim=180 for front view\n",
    "\n",
    "        # Collect all coordinates to determine dynamic axis limits\n",
    "        all_x_3d, all_y_3d, all_z_3d = [], [], []\n",
    "        for pose3d in poses3d:\n",
    "            # Adjust axis transformation: swap Y and Z, negate Z for height, and ensure front faces viewer\n",
    "            pose3d_adjusted = pose3d.copy()\n",
    "            # New transformation: X (width), Y (height), Z (depth), with front facing positive Z\n",
    "            pose3d_adjusted[:, [1, 2]] = pose3d[:, [2, 1]]  # Swap Y and Z\n",
    "            pose3d_adjusted[:, 2] = -pose3d_adjusted[:, 2]  # Negate Z for depth (front faces viewer)\n",
    "            all_x_3d.extend(pose3d_adjusted[:, 0])\n",
    "            all_y_3d.extend(pose3d_adjusted[:, 1])\n",
    "            all_z_3d.extend(pose3d_adjusted[:, 2])\n",
    "\n",
    "        # Set dynamic axis limits with padding and equal aspect ratio\n",
    "        if all_x_3d:\n",
    "            x_min, x_max = min(all_x_3d), max(all_x_3d)\n",
    "            y_min, y_max = min(all_y_3d), max(all_y_3d)\n",
    "            z_min, z_max = min(all_z_3d), max(all_z_3d)\n",
    "            padding = 0.5  # Add padding in meters\n",
    "            ax.set_xlim(x_min - padding, x_max + padding)\n",
    "            ax.set_ylim(y_min - padding, y_max + padding)\n",
    "            ax.set_zlim(z_min - padding, z_max + padding)\n",
    "            # Ensure equal aspect ratio for undistorted 3D view\n",
    "            ax.set_box_aspect([1, 1, 1])  # Equal scaling on all axes\n",
    "        else:\n",
    "            ax.set_xlim(-1.5, 1.5)\n",
    "            ax.set_ylim(-1.5, 1.5)\n",
    "            ax.set_zlim(0, 3.0)\n",
    "            ax.set_box_aspect([1, 1, 1])  # Default equal aspect ratio\n",
    "\n",
    "        ax.set_xlabel('X (m)')\n",
    "        ax.set_ylabel('Y (m)')\n",
    "        ax.set_zlabel('Z (m)')\n",
    "\n",
    "        # Plot 3D skeleton for each person\n",
    "        for pose3d, person_id in zip(poses3d, person_ids):\n",
    "            if person_id == -1:\n",
    "                continue\n",
    "            # Apply axis transformation for plotting\n",
    "            pose3d_adjusted = pose3d.copy()\n",
    "            pose3d_adjusted[:, [1, 2]] = pose3d[:, [2, 1]]  # Swap Y and Z\n",
    "            pose3d_adjusted[:, 2] = -pose3d_adjusted[:, 2]  # Negate Z for depth\n",
    "            # Plot skeleton connections\n",
    "            for i_start, i_end in skeleton_edges:\n",
    "                ax.plot(*zip(pose3d_adjusted[i_start], pose3d_adjusted[i_end]), 'g-', marker='o', markersize=2)\n",
    "            ax.scatter(*pose3d_adjusted.T, c='g', s=2)\n",
    "            # Add person label above the head (first joint, e.g., pelvis)\n",
    "            ax.text(pose3d_adjusted[0, 0], pose3d_adjusted[0, 1], pose3d_adjusted[0, 2] + 0.2, \n",
    "                    f\"Person {person_id + 1}\", color='red')\n",
    "\n",
    "        fig.tight_layout()\n",
    "        # Save the plot as an image\n",
    "        temp_image_path = os.path.join(temp_dir, f\"3d_frame_{frame_idx}.png\")\n",
    "        plt.savefig(temp_image_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        return temp_image_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error rendering 3D skeleton for frame {frame_idx}: {str(e)}\")\n",
    "        return None\n",
    "#def render_3d_skeleton(poses3d, person_ids, temp_dir, frame_idx, frame_width, frame_height, skeleton_edges):\n",
    "    \"\"\"Render the 3D skeleton as an image for a single frame.\"\"\"\n",
    "    try:\n",
    "        # Calculate aspect ratio and set figure size based on video dimensions\n",
    "        aspect_ratio = frame_width / frame_height\n",
    "        fig_width = 10  # Base width in inches\n",
    "        fig_height = fig_width / aspect_ratio  # Adjust height to match aspect ratio\n",
    "        fig = plt.figure(figsize=(fig_width, fig_height), dpi=150)  # Higher DPI for better quality\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "        # Set view angle for a front-facing perspective\n",
    "        ax.view_init(elev=15, azim=180)  # Front view, as corrected previously\n",
    "\n",
    "        # Collect all coordinates to determine dynamic axis limits\n",
    "        all_x_3d, all_y_3d, all_z_3d = [], [], []\n",
    "        for pose3d in poses3d:\n",
    "            # Adjust axis transformation: swap Y and Z, negate Z for height, ensure front faces viewer\n",
    "            pose3d_adjusted = pose3d.copy()\n",
    "            pose3d_adjusted[:, [1, 2]] = pose3d[:, [2, 1]]  # Swap Y and Z\n",
    "            pose3d_adjusted[:, 2] = -pose3d_adjusted[:, 2]  # Negate Z for depth\n",
    "            all_x_3d.extend(pose3d_adjusted[:, 0])\n",
    "            all_y_3d.extend(pose3d_adjusted[:, 1])\n",
    "            all_z_3d.extend(pose3d_adjusted[:, 2])\n",
    "\n",
    "        # Set dynamic axis limits with padding and equal aspect ratio\n",
    "        if all_x_3d:\n",
    "            x_min, x_max = min(all_x_3d), max(all_x_3d)\n",
    "            y_min, y_max = min(all_y_3d), max(all_y_3d)\n",
    "            z_min, z_max = min(all_z_3d), max(all_z_3d)\n",
    "            padding = 0.5  # Add padding in meters\n",
    "            ax.set_xlim(x_min - padding, x_max + padding)\n",
    "            ax.set_ylim(y_min - padding, y_max + padding)\n",
    "            ax.set_zlim(z_min - padding, z_max + padding)\n",
    "            ax.set_box_aspect([1, 1, 1])  # Equal scaling on all axes\n",
    "        else:\n",
    "            ax.set_xlim(-1.5, 1.5)\n",
    "            ax.set_ylim(-1.5, 1.5)\n",
    "            ax.set_zlim(0, 3.0)\n",
    "            ax.set_box_aspect([1, 1, 1])  # Default equal aspect ratio\n",
    "\n",
    "        ax.set_xlabel('X (m)')\n",
    "        ax.set_ylabel('Y (m)')\n",
    "        ax.set_zlabel('Z (m)')\n",
    "\n",
    "        # Plot 3D skeleton for each person\n",
    "        for pose3d, person_id in zip(poses3d, person_ids):\n",
    "            if person_id == -1:\n",
    "                continue\n",
    "            # Apply axis transformation for plotting\n",
    "            pose3d_adjusted = pose3d.copy()\n",
    "            pose3d_adjusted[:, [1, 2]] = pose3d[:, [2, 1]]  # Swap Y and Z\n",
    "            pose3d_adjusted[:, 2] = -pose3d_adjusted[:, 2]  # Negate Z for depth\n",
    "            # Plot skeleton connections\n",
    "            for i_start, i_end in skeleton_edges:\n",
    "                ax.plot(*zip(pose3d_adjusted[i_start], pose3d_adjusted[i_end]), 'g-', marker='o', markersize=2)\n",
    "            ax.scatter(*pose3d_adjusted.T, c='g', s=2)\n",
    "            # Add person label above the head (first joint, e.g., pelvis)\n",
    "            ax.text(pose3d_adjusted[0, 0], pose3d_adjusted[0, 1], pose3d_adjusted[0, 2] + 0.2, \n",
    "                    f\"Person {person_id + 1}\", color='red')\n",
    "\n",
    "        fig.tight_layout()\n",
    "        # Save the plot at the target resolution without resizing\n",
    "        temp_image_path = os.path.join(temp_dir, f\"3d_frame_{frame_idx}.png\")\n",
    "        plt.savefig(temp_image_path, dpi=150, bbox_inches='tight')  # Match video DPI\n",
    "        plt.close()\n",
    "\n",
    "        return temp_image_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error rendering 3D skeleton for frame {frame_idx}: {str(e)}\")\n",
    "        return None\n",
    "def main():\n",
    "    # Paths\n",
    "    zip_path = \"C:\\\\Users\\\\akhileshsing2024\\\\Downloads\\\\metrabs_mob3l_y4t.zip\"\n",
    "    video_path = \"C:\\\\Users\\\\akhileshsing2024\\\\Downloads\\\\Mediapipe\\\\walk.mp4\"\n",
    "    output_excel = \"2dand3dposedatawithlabelschanged.xlsx\"\n",
    "\n",
    "    # Extract and load the model\n",
    "    model_path = extract_model(zip_path)\n",
    "    model = tf.saved_model.load(model_path)\n",
    "    print(\"Model loaded successfully!\")\n",
    "\n",
    "    # Load skeleton information\n",
    "    skeleton = 'smpl_24'\n",
    "    joint_names = model.per_skeleton_joint_names[skeleton].numpy().astype(str)\n",
    "    joint_edges = model.per_skeleton_joint_edges[skeleton].numpy()\n",
    "\n",
    "    # Process video (this generates the initial Excel file with 2D in pixels, 3D in mm)\n",
    "    process_video(video_path, model, joint_edges, output_excel=output_excel)\n",
    "\n",
    "    # Load the generated Excel file\n",
    "    df = pd.read_excel(output_excel)\n",
    "\n",
    "    # Step 1: Compute pixel-to-mm conversion factor using hip-to-ankle distance\n",
    "    unique_persons = df['Person'].unique()\n",
    "    pixel_to_mm_factors = []\n",
    "\n",
    "    for person_id in unique_persons:\n",
    "        df_person = df[df['Person'] == person_id]\n",
    "        \n",
    "        # Extract hip and ankle coordinates for all frames\n",
    "        left_hip_2d = df_person[df_person['Joint'] == 1][['Frame', 'X_2D', 'Y_2D']]\n",
    "        left_ankle_2d = df_person[df_person['Joint'] == 7][['Frame', 'X_2D', 'Y_2D']]\n",
    "        left_hip_3d = df_person[df_person['Joint'] == 1][['Frame', 'X_3D', 'Y_3D', 'Z_3D']]\n",
    "        left_ankle_3d = df_person[df_person['Joint'] == 7][['Frame', 'X_3D', 'Y_3D', 'Z_3D']]\n",
    "\n",
    "        # Merge on Frame to get corresponding hip and ankle coordinates\n",
    "        hip_ankle_2d = left_hip_2d.merge(left_ankle_2d, on='Frame', suffixes=('_hip', '_ankle'))\n",
    "        hip_ankle_3d = left_hip_3d.merge(left_ankle_3d, on='Frame', suffixes=('_hip', '_ankle'))\n",
    "\n",
    "        # Compute distances for each frame\n",
    "        distances_2d = []\n",
    "        distances_3d = []\n",
    "        for idx in range(len(hip_ankle_2d)):\n",
    "            # 2D distance (pixels)\n",
    "            hip_2d = hip_ankle_2d.iloc[idx][['X_2D_hip', 'Y_2D_hip']].values\n",
    "            ankle_2d = hip_ankle_2d.iloc[idx][['X_2D_ankle', 'Y_2D_ankle']].values\n",
    "            dist_2d = distance.euclidean(hip_2d, ankle_2d)\n",
    "\n",
    "            # 3D distance (mm)\n",
    "            hip_3d = hip_ankle_3d.iloc[idx][['X_3D_hip', 'Y_3D_hip', 'Z_3D_hip']].values\n",
    "            ankle_3d = hip_ankle_3d.iloc[idx][['X_3D_ankle', 'Y_3D_ankle', 'Z_3D_ankle']].values\n",
    "            dist_3d = distance.euclidean(hip_3d, ankle_3d)\n",
    "\n",
    "            if dist_2d > 0:  # Avoid division by zero\n",
    "                pixel_to_mm = dist_3d / dist_2d\n",
    "                pixel_to_mm_factors.append(pixel_to_mm)\n",
    "                distances_2d.append(dist_2d)\n",
    "                distances_3d.append(dist_3d)\n",
    "\n",
    "    # Compute the average pixel-to-mm factor\n",
    "    if pixel_to_mm_factors:\n",
    "        pixel_to_mm = np.mean(pixel_to_mm_factors)\n",
    "        print(f\"Computed pixel-to-mm conversion factor: {pixel_to_mm:.4f} mm/pixel\")\n",
    "        print(f\"Average 3D hip-to-ankle distance: {np.mean(distances_3d):.2f} mm\")\n",
    "        print(f\"Average 2D hip-to-ankle distance: {np.mean(distances_2d):.2f} pixels\")\n",
    "    else:\n",
    "        # Fallback to anthropometric average if no valid distances are computed\n",
    "        pixel_to_mm = 900 / 500  # Assume 900 mm leg length, 500 pixels in 2D (rough estimate)\n",
    "        print(f\"No valid distances computed. Using fallback pixel-to-mm factor: {pixel_to_mm:.4f} mm/pixel\")\n",
    "\n",
    "    # Step 2: Convert 2D coordinates to millimeters\n",
    "    df['X_2D'] = df['X_2D'] * pixel_to_mm\n",
    "    df['Y_2D'] = df['Y_2D'] * pixel_to_mm\n",
    "\n",
    "    # Step 3: Save the updated Excel file with 2D and 3D coordinates in millimeters\n",
    "    updated_excel = \"2dand3dposedatawithlabelschanged_converted.xlsx\"\n",
    "    df.to_excel(updated_excel, index=False)\n",
    "    print(f\"Updated pose data with 2D coordinates in millimeters saved as {updated_excel}\")\n",
    "\n",
    "    # Step 4: Calculate gait metrics\n",
    "    fps = cv2.VideoCapture(video_path).get(cv2.CAP_PROP_FPS)\n",
    "    print(f\"Detected {len(unique_persons)} persons in the video.\")\n",
    "\n",
    "    metrics_2d_all = []\n",
    "    metrics_3d_all = []\n",
    "    for person_id in unique_persons:\n",
    "        print(f\"\\nProcessing Person {person_id}...\")\n",
    "        \n",
    "        # 2D Metrics (now in millimeters)\n",
    "        metrics_2d = calculate_gait_metrics_2d(df, person_id, fps, joint_edges, joint_names)\n",
    "        if metrics_2d:\n",
    "            metrics_2d_all.append(metrics_2d)\n",
    "            print(f\"2D Metrics for Person {person_id}:\")\n",
    "            for key, value in metrics_2d.items():\n",
    "                if key != 'Person':\n",
    "                    print(f\"{key}: {value:.2f}\" if not np.isnan(value) else f\"{key}: N/A\")\n",
    "\n",
    "        # 3D Metrics (already in millimeters)\n",
    "        metrics_3d = calculate_gait_metrics_3d(df, person_id, fps, joint_edges, joint_names)\n",
    "        if metrics_3d:\n",
    "            metrics_3d_all.append(metrics_3d)\n",
    "            print(f\"3D Metrics for Person {person_id}:\")\n",
    "            for key, value in metrics_3d.items():\n",
    "                if key != 'Person':\n",
    "                    print(f\"{key}: {value:.2f}\" if not np.isnan(value) else f\"{key}: N/A\")\n",
    "\n",
    "    # Save to Excel with separate sheets for 2D and 3D\n",
    "    metrics_df_2d = pd.DataFrame(metrics_2d_all)\n",
    "    metrics_df_3d = pd.DataFrame(metrics_3d_all)\n",
    "    with pd.ExcelWriter(\"gait_metrics_per_person.xlsx\") as writer:\n",
    "        metrics_df_2d.to_excel(writer, sheet_name='2D Metrics', index=False)\n",
    "        metrics_df_3d.to_excel(writer, sheet_name='3D Metrics', index=False)\n",
    "    print(\"\\nGait metrics for all detected persons saved to 'gait_metrics_per_person.xlsx'\")\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f424fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "No pose detected in frame 25\n",
      "No pose detected in frame 26\n",
      "No pose detected in frame 27\n",
      "No pose detected in frame 36\n",
      "No pose detected in frame 37\n",
      "No pose detected in frame 129\n",
      "No pose detected in frame 130\n",
      "No pose detected in frame 142\n",
      "Processed 2D video saved as output_video3d_withlabelschanged.mp4\n",
      "Processed 3D video saved as output_3d_pose.mp4\n",
      "Pose data saved as 2dand3dposedatawithlabelschanged.xlsx\n",
      "Computed pixel-to-mm conversion factor: 2001.6513 mm/pixel\n",
      "Updated pose data saved as 2dand3dposedatawithlabelschanged_converted.xlsx\n",
      "Gait metrics saved to 'gait_metrics_per_person.xlsx'\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import urllib.request\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as tfhub\n",
    "import tensorflow_io as tfio\n",
    "import cv2\n",
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial import distance\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import shutil\n",
    "\n",
    "def extract_model(zip_path, extract_to=\"models\"):\n",
    "    \"\"\"Extracts a zip file and returns the extracted directory path.\"\"\"\n",
    "    extract_dir = os.path.join(os.path.dirname(zip_path), extract_to)\n",
    "    if not os.path.exists(extract_dir):\n",
    "        os.makedirs(extract_dir)\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "    model_dir = os.path.join(extract_dir, os.path.splitext(os.path.basename(zip_path))[0])\n",
    "    return model_dir\n",
    "\n",
    "def draw_skeleton(frame, keypoints, skeleton_edges):\n",
    "    \"\"\"Draws skeleton on the frame using detected keypoints.\"\"\"\n",
    "    for (i, j) in skeleton_edges:\n",
    "        pt1, pt2 = keypoints[i], keypoints[j]\n",
    "        if keypoints.shape[-1] == 3:  \n",
    "            if (pt1[2] > 0.5) and (pt2[2] > 0.5):\n",
    "                cv2.line(frame, (int(pt1[0]), int(pt1[1])), (int(pt2[0]), int(pt2[1])), (255, 0, 0), 2)\n",
    "        else:\n",
    "            cv2.line(frame, (int(pt1[0]), int(pt1[1])), (int(pt2[0]), int(pt2[1])), (255, 0, 0), 2)\n",
    "    return frame\n",
    "\n",
    "def visualize(im, detections, poses3d, poses2d, edges, frame_idx, plot_dir, display=False):\n",
    "    \"\"\"Visualize 2D and 3D poses with bounding boxes and save to plot_dir.\"\"\"\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "    fig = plt.figure(figsize=(10, 5.2))\n",
    "    image_ax = fig.add_subplot(1, 2, 1)\n",
    "    image_ax.imshow(im)\n",
    "    for x, y, w, h in detections[:, :4]:\n",
    "        image_ax.add_patch(Rectangle((x, y), w, h, fill=False, edgecolor='r'))\n",
    "    \n",
    "    for pose2d in poses2d:\n",
    "        for i_start, i_end in edges:\n",
    "            image_ax.plot(*zip(pose2d[i_start], pose2d[i_end]), 'b-', marker='o', markersize=2)\n",
    "        image_ax.scatter(*pose2d.T, c='r', s=2)\n",
    "    \n",
    "    pose_ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "    pose_ax.view_init(5, -85)\n",
    "    \n",
    "    all_x_3d, all_y_3d, all_z_3d = [], [], []\n",
    "    for pose3d in poses3d:\n",
    "        x_3d = pose3d[:, 0] * 1000  # Convert meters to mm\n",
    "        y_3d = pose3d[:, 2] * 1000  # Swap y and z, convert to mm\n",
    "        z_3d = -pose3d[:, 1] * 1000  # Negate and convert to mm\n",
    "        all_x_3d.extend(x_3d)\n",
    "        all_y_3d.extend(y_3d)\n",
    "        all_z_3d.extend(z_3d)\n",
    "    \n",
    "    if all_x_3d:\n",
    "        padding = 500\n",
    "        pose_ax.set_xlim(min(all_x_3d) - padding, max(all_x_3d) + padding)\n",
    "        pose_ax.set_ylim(min(all_y_3d) - padding, max(all_y_3d) + padding)\n",
    "        pose_ax.set_zlim(min(all_z_3d) - padding, max(all_z_3d) + padding)\n",
    "    else:\n",
    "        pose_ax.set_xlim(-1500, 1500)\n",
    "        pose_ax.set_ylim(-1500, 1500)\n",
    "        pose_ax.set_zlim(0, 3000)\n",
    "    \n",
    "    pose_ax.set_xlabel('X (mm)')\n",
    "    pose_ax.set_ylabel('Y (mm)')\n",
    "    pose_ax.set_zlabel('Z (mm)')\n",
    "    \n",
    "    for pose3d in poses3d:\n",
    "        x_3d = pose3d[:, 0] * 1000\n",
    "        y_3d = pose3d[:, 2] * 1000\n",
    "        z_3d = -pose3d[:, 1] * 1000\n",
    "        for i_start, i_end in edges:\n",
    "            pose_ax.plot([x_3d[i_start], x_3d[i_end]], [y_3d[i_start], y_3d[i_end]], \n",
    "                         [z_3d[i_start], z_3d[i_end]], 'g-', marker='o', markersize=2)\n",
    "        pose_ax.scatter(x_3d, y_3d, z_3d, c='g', s=2)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plot_path = os.path.join(plot_dir, f\"side_by_side_frame_{frame_idx}.png\")\n",
    "    plt.savefig(plot_path)\n",
    "    if display:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "def render_3d_skeleton(poses3d, person_ids, edges, temp_dir, frame_idx, frame_width, frame_height):\n",
    "    \"\"\"Render 3D skeleton as an image for a single frame.\"\"\"\n",
    "    try:\n",
    "        os.makedirs(temp_dir, exist_ok=True)\n",
    "        fig = plt.figure(figsize=(frame_width / 100, frame_height / 100), dpi=100)\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.view_init(5, -85)\n",
    "        \n",
    "        all_x_3d, all_y_3d, all_z_3d = [], [], []\n",
    "        for pose3d in poses3d:\n",
    "            x_3d = pose3d[:, 0] * 1000\n",
    "            y_3d = pose3d[:, 2] * 1000\n",
    "            z_3d = -pose3d[:, 1] * 1000\n",
    "            all_x_3d.extend(x_3d)\n",
    "            all_y_3d.extend(y_3d)\n",
    "            all_z_3d.extend(z_3d)\n",
    "        \n",
    "        if all_x_3d:\n",
    "            padding = 500\n",
    "            ax.set_xlim(min(all_x_3d) - padding, max(all_x_3d) + padding)\n",
    "            ax.set_ylim(min(all_y_3d) - padding, max(all_y_3d) + padding)\n",
    "            ax.set_zlim(min(all_z_3d) - padding, max(all_z_3d) + padding)\n",
    "        else:\n",
    "            ax.set_xlim(-1500, 1500)\n",
    "            ax.set_ylim(-1500, 1500)\n",
    "            ax.set_zlim(0, 3000)\n",
    "        \n",
    "        ax.set_xlabel('X (mm)')\n",
    "        ax.set_ylabel('Y (mm)')\n",
    "        ax.set_zlabel('Z (mm)')\n",
    "        \n",
    "        for pose3d, person_id in zip(poses3d, person_ids):\n",
    "            if person_id == -1:\n",
    "                continue\n",
    "            x_3d = pose3d[:, 0] * 1000\n",
    "            y_3d = pose3d[:, 2] * 1000\n",
    "            z_3d = -pose3d[:, 1] * 1000\n",
    "            for i_start, i_end in edges:\n",
    "                ax.plot([x_3d[i_start], x_3d[i_end]], [y_3d[i_start], y_3d[i_end]], \n",
    "                        [z_3d[i_start], z_3d[i_end]], 'g-', marker='o', markersize=2)\n",
    "            ax.scatter(x_3d, y_3d, z_3d, c='g', s=2)\n",
    "            # Add person label above head (joint 0: pelvis)\n",
    "            ax.text(x_3d[0], y_3d[0], z_3d[0] + 200, f\"Person {person_id + 1}\", color='red')\n",
    "        \n",
    "        fig.tight_layout()\n",
    "        temp_image_path = os.path.join(temp_dir, f\"3d_frame_{frame_idx}.png\")\n",
    "        plt.savefig(temp_image_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        return temp_image_path\n",
    "    except Exception as e:\n",
    "        print(f\"Error rendering 3D skeleton for frame {frame_idx}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_video(video_path, model, skeleton_edges, output_2d_video=\"output_video3d_withlabelschanged.mp4\", \n",
    "                 output_3d_video=\"output_3d_pose.mp4\", frame_interval=10, output_excel=\"2dand3dposedatawithlabelschanged.xlsx\"):\n",
    "    \"\"\"Process video frames with pose estimation, overlay skeletons and person labels, generate 2D and 3D videos, and save coordinates.\"\"\"\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(f\"Cannot open video file: {video_path}\")\n",
    "    frame_width = int(cap.get(3))\n",
    "    frame_height = int(cap.get(4))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    if fps <= 0:\n",
    "        fps = 30\n",
    "        print(\"Invalid FPS. Using default FPS of 30.\")\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out_2d = cv2.VideoWriter(output_2d_video, fourcc, fps, (frame_width, frame_height))\n",
    "    out_3d = cv2.VideoWriter(output_3d_video, fourcc, fps, (frame_width, frame_height))\n",
    "    if not out_2d.isOpened() or not out_3d.isOpened():\n",
    "        raise ValueError(\"Could not initialize VideoWriter. Check codec or permissions.\")\n",
    "\n",
    "    temp_dir = \"temp_3d_frames\"\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    \n",
    "    frame_count = 0\n",
    "    data_list = []\n",
    "    plot_dir = \"3d_plots\"\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        original_height, original_width = frame.shape[:2]\n",
    "        frame_rgb_resized = cv2.resize(frame_rgb, (640, 480))\n",
    "        image_tensor = tf.convert_to_tensor(frame_rgb_resized, dtype=tf.uint8)\n",
    "\n",
    "        pred = model.detect_poses(image_tensor, skeleton='smpl_24')\n",
    "        person_ids = list(range(pred['poses2d'].shape[0]))  # Simple person ID assignment\n",
    "\n",
    "        if 'poses2d' in pred and 'poses3d' in pred:\n",
    "            keypoints_2d = pred['poses2d'].numpy()\n",
    "            keypoints_3d = pred['poses3d'].numpy()\n",
    "            boxes = pred.get('boxes', tf.zeros((keypoints_2d.shape[0], 4))).numpy()\n",
    "\n",
    "            if keypoints_2d.shape[0] == 0:\n",
    "                print(f\"No pose detected in frame {frame_count}\")\n",
    "                out_2d.write(frame)\n",
    "                blank_frame = np.zeros((frame_height, frame_width, 3), dtype=np.uint8)\n",
    "                out_3d.write(blank_frame)\n",
    "            else:\n",
    "                if frame_count % frame_interval == 0:\n",
    "                    visualize(frame_rgb_resized, boxes, keypoints_3d, keypoints_2d[:, :, :2], \n",
    "                             skeleton_edges, frame_count, plot_dir, display=False)\n",
    "\n",
    "                for i in range(keypoints_2d.shape[0]):\n",
    "                    if keypoints_2d[i].shape[0] > 0 and keypoints_3d[i].shape[0] > 0:\n",
    "                        kpt_2d = keypoints_2d[i]\n",
    "                        kpt_3d = keypoints_3d[i]\n",
    "                        kpt_2d[:, 0] *= original_width / 640\n",
    "                        kpt_2d[:, 1] *= original_height / 480\n",
    "                        frame = draw_skeleton(frame, kpt_2d, skeleton_edges)\n",
    "                        person_label = f\"Person {i + 1}\"\n",
    "                        label_position = (int(kpt_2d[0][0]), int(kpt_2d[0][1] - 20))\n",
    "                        cv2.putText(frame, person_label, label_position, \n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "                        \n",
    "                        for j in range(min(kpt_2d.shape[0], kpt_3d.shape[0])):\n",
    "                            data_list.append([\n",
    "                                frame_count, i, j, \n",
    "                                kpt_2d[j][0], kpt_2d[j][1], \n",
    "                                kpt_3d[j][0] * 1000, kpt_3d[j][1] * 1000, kpt_3d[j][2] * 1000\n",
    "                            ])\n",
    "                \n",
    "                out_2d.write(frame)\n",
    "                \n",
    "                temp_image_path = render_3d_skeleton(keypoints_3d, person_ids, skeleton_edges, \n",
    "                                                   temp_dir, frame_count, frame_width, frame_height)\n",
    "                if temp_image_path:\n",
    "                    plot_image = cv2.imread(temp_image_path)\n",
    "                    if plot_image is not None:\n",
    "                        plot_image = cv2.resize(plot_image, (frame_width, frame_height))\n",
    "                        out_3d.write(plot_image)\n",
    "                    else:\n",
    "                        print(f\"Failed to read 3D plot image for frame {frame_count}\")\n",
    "                        out_3d.write(np.zeros((frame_height, frame_width, 3), dtype=np.uint8))\n",
    "                else:\n",
    "                    out_3d.write(np.zeros((frame_height, frame_width, 3), dtype=np.uint8))\n",
    "        else:\n",
    "            out_2d.write(frame)\n",
    "            out_3d.write(np.zeros((frame_height, frame_width, 3), dtype=np.uint8))\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    out_2d.release()\n",
    "    out_3d.release()\n",
    "    shutil.rmtree(temp_dir)\n",
    "    print(f\"Processed 2D video saved as {output_2d_video}\")\n",
    "    print(f\"Processed 3D video saved as {output_3d_video}\")\n",
    "\n",
    "    df = pd.DataFrame(data_list, columns=['Frame', 'Person', 'Joint', 'X_2D', 'Y_2D', 'X_3D', 'Y_3D', 'Z_3D'])\n",
    "    df.to_excel(output_excel, index=False)\n",
    "    print(f\"Pose data saved as {output_excel}\")\n",
    "    return df\n",
    "\n",
    "def calculate_gait_metrics_2d(df, person_id, fps, skeleton_edges, joint_names):\n",
    "    \"\"\"Calculate 2D gait metrics for a specific person.\"\"\"\n",
    "    joint_map = {\n",
    "        'pelvis': 0, 'left_hip': 1, 'right_hip': 2, 'spine1': 3,\n",
    "        'left_knee': 4, 'right_knee': 5, 'spine2': 6,\n",
    "        'left_ankle': 7, 'right_ankle': 8\n",
    "    }\n",
    "    df_person = df[df['Person'] == person_id]\n",
    "    if df_person.empty:\n",
    "        return None\n",
    "    left_ankle_2d = df_person[df_person['Joint'] == joint_map['left_ankle']][['Frame', 'X_2D', 'Y_2D']].values\n",
    "    right_ankle_2d = df_person[df_person['Joint'] == joint_map['right_ankle']][['Frame', 'X_2D', 'Y_2D']].values\n",
    "    left_strikes_2d = []\n",
    "    right_strikes_2d = []\n",
    "    for i in range(1, len(left_ankle_2d) - 1):\n",
    "        if left_ankle_2d[i][2] < left_ankle_2d[i-1][2] and left_ankle_2d[i][2] < left_ankle_2d[i+1][2]:\n",
    "            left_strikes_2d.append(left_ankle_2d[i])\n",
    "        if right_ankle_2d[i][2] < right_ankle_2d[i-1][2] and right_ankle_2d[i][2] < right_ankle_2d[i+1][2]:\n",
    "            right_strikes_2d.append(right_ankle_2d[i])\n",
    "    left_strikes_2d = np.array(left_strikes_2d)\n",
    "    right_strikes_2d = np.array(right_strikes_2d)\n",
    "    stride_time_left_2d = np.mean(np.diff(left_strikes_2d[:, 0]) / fps) if len(left_strikes_2d) > 1 else np.nan\n",
    "    stride_time_right_2d = np.mean(np.diff(right_strikes_2d[:, 0]) / fps) if len(right_strikes_2d) > 1 else np.nan\n",
    "    stride_length_left_2d = np.mean([distance.euclidean(left_strikes_2d[i][1:3], left_strikes_2d[i+1][1:3]) \n",
    "                                    for i in range(len(left_strikes_2d)-1)]) if len(left_strikes_2d) > 1 else np.nan\n",
    "    stride_length_right_2d = np.mean([distance.euclidean(right_strikes_2d[i][1:3], right_strikes_2d[i+1][1:3]) \n",
    "                                     for i in range(len(right_strikes_2d)-1)]) if len(right_strikes_2d) > 1 else np.nan\n",
    "    total_steps_2d = len(left_strikes_2d) + len(right_strikes_2d)\n",
    "    total_time_2d = (df_person['Frame'].max() - df_person['Frame'].min()) / fps / 60 if not df_person['Frame'].empty else 0\n",
    "    cadence_2d = total_steps_2d / total_time_2d if total_time_2d > 0 else np.nan\n",
    "    gait_speed_left_2d = stride_length_left_2d / stride_time_left_2d if stride_time_left_2d > 0 else np.nan\n",
    "    gait_speed_right_2d = stride_length_right_2d / stride_time_right_2d if stride_time_right_2d > 0 else np.nan\n",
    "    angles_2d = {'left_knee': [], 'right_knee': [], 'left_hip': [], 'right_hip': []}\n",
    "    for frame in df_person['Frame'].unique():\n",
    "        frame_data = df_person[df_person['Frame'] == frame]\n",
    "        def get_coords_2d(joint):\n",
    "            row = frame_data[frame_data['Joint'] == joint_map[joint]]\n",
    "            return row[['X_2D', 'Y_2D']].values[0] if not row.empty else np.array([np.nan, np.nan])\n",
    "        left_hip_2d = get_coords_2d('left_hip')\n",
    "        left_knee_2d = get_coords_2d('left_knee')\n",
    "        left_ankle_2d = get_coords_2d('left_ankle')\n",
    "        right_hip_2d = get_coords_2d('right_hip')\n",
    "        right_knee_2d = get_coords_2d('right_knee')\n",
    "        right_ankle_2d = get_coords_2d('right_ankle')\n",
    "        spine_2d = get_coords_2d('spine1')\n",
    "        def angle_between_2d(v1, v2):\n",
    "            if np.any(np.isnan(v1)) or np.any(np.isnan(v2)):\n",
    "                return np.nan\n",
    "            cos_theta = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "            return np.degrees(np.arccos(np.clip(cos_theta, -1.0, 1.0)))\n",
    "        angles_2d['left_knee'].append(angle_between_2d(left_hip_2d - left_knee_2d, left_ankle_2d - left_knee_2d))\n",
    "        angles_2d['right_knee'].append(angle_between_2d(right_hip_2d - right_knee_2d, right_ankle_2d - right_knee_2d))\n",
    "        angles_2d['left_hip'].append(angle_between_2d(spine_2d - left_hip_2d, left_knee_2d - left_hip_2d))\n",
    "        angles_2d['right_hip'].append(angle_between_2d(spine_2d - right_hip_2d, right_knee_2d - right_hip_2d))\n",
    "    avg_angles_2d = {k: np.nanmean(v) for k, v in angles_2d.items()}\n",
    "    return {\n",
    "        'Person': person_id,\n",
    "        'Stride Time Left (s)': stride_time_left_2d,\n",
    "        'Stride Time Right (s)': stride_time_right_2d,\n",
    "        'Stride Length Left (mm)': stride_length_left_2d,\n",
    "        'Stride Length Right (mm)': stride_length_right_2d,\n",
    "        'Cadence (steps/min)': cadence_2d,\n",
    "        'Gait Speed Left (mm/s)': gait_speed_left_2d,\n",
    "        'Gait Speed Right (mm/s)': gait_speed_right_2d,\n",
    "        'Avg Left Knee Angle (deg)': avg_angles_2d['left_knee'],\n",
    "        'Avg Right Knee Angle (deg)': avg_angles_2d['right_knee'],\n",
    "        'Avg Left Hip Angle (deg)': avg_angles_2d['left_hip'],\n",
    "        'Avg Right Hip Angle (deg)': avg_angles_2d['right_hip']\n",
    "    }\n",
    "\n",
    "def calculate_gait_metrics_3d(df, person_id, fps, skeleton_edges, joint_names):\n",
    "    \"\"\"Calculate 3D gait metrics for a specific person.\"\"\"\n",
    "    joint_map = {\n",
    "        'pelvis': 0, 'left_hip': 1, 'right_hip': 2, 'spine1': 3,\n",
    "        'left_knee': 4, 'right_knee': 5, 'spine2': 6,\n",
    "        'left_ankle': 7, 'right_ankle': 8\n",
    "    }\n",
    "    df_person = df[df['Person'] == person_id]\n",
    "    if df_person.empty:\n",
    "        return None\n",
    "    left_ankle_3d = df_person[df_person['Joint'] == joint_map['left_ankle']][['Frame', 'X_3D', 'Y_3D', 'Z_3D']].values\n",
    "    right_ankle_3d = df_person[df_person['Joint'] == joint_map['right_ankle']][['Frame', 'X_3D', 'Y_3D', 'Z_3D']].values\n",
    "    left_strikes_3d = []\n",
    "    right_strikes_3d = []\n",
    "    for i in range(1, len(left_ankle_3d) - 1):\n",
    "        if left_ankle_3d[i][3] < left_ankle_3d[i-1][3] and left_ankle_3d[i][3] < left_ankle_3d[i+1][3]:\n",
    "            left_strikes_3d.append(left_ankle_3d[i])\n",
    "        if right_ankle_3d[i][3] < right_ankle_3d[i-1][3] and right_ankle_3d[i][3] < right_ankle_3d[i+1][3]:\n",
    "            right_strikes_3d.append(right_ankle_3d[i])\n",
    "    left_strikes_3d = np.array(left_strikes_3d)\n",
    "    right_strikes_3d = np.array(right_strikes_3d)\n",
    "    stride_time_left_3d = np.mean(np.diff(left_strikes_3d[:, 0]) / fps) if len(left_strikes_3d) > 1 else np.nan\n",
    "    stride_time_right_3d = np.mean(np.diff(right_strikes_3d[:, 0]) / fps) if len(right_strikes_3d) > 1 else np.nan\n",
    "    stride_length_left_3d = np.mean([distance.euclidean(left_strikes_3d[i][1:4], left_strikes_3d[i+1][1:4]) \n",
    "                                    for i in range(len(left_strikes_3d)-1)]) if len(left_strikes_3d) > 1 else np.nan\n",
    "    stride_length_right_3d = np.mean([distance.euclidean(right_strikes_3d[i][1:4], right_strikes_3d[i+1][1:4]) \n",
    "                                     for i in range(len(right_strikes_3d)-1)]) if len(right_strikes_3d) > 1 else np.nan\n",
    "    total_steps_3d = len(left_strikes_3d) + len(right_strikes_3d)\n",
    "    total_time_3d = (df_person['Frame'].max() - df_person['Frame'].min()) / fps / 60 if not df_person['Frame'].empty else 0\n",
    "    cadence_3d = total_steps_3d / total_time_3d if total_time_3d > 0 else np.nan\n",
    "    gait_speed_left_3d = stride_length_left_3d / stride_time_left_3d if stride_time_left_3d > 0 else np.nan\n",
    "    gait_speed_right_3d = stride_length_right_3d / stride_time_right_3d if stride_time_right_3d > 0 else np.nan\n",
    "    angles_3d = {'left_knee': [], 'right_knee': [], 'left_hip': [], 'right_hip': []}\n",
    "    for frame in df_person['Frame'].unique():\n",
    "        frame_data = df_person[df_person['Frame'] == frame]\n",
    "        def get_coords_3d(joint):\n",
    "            row = frame_data[frame_data['Joint'] == joint_map[joint]]\n",
    "            return row[['X_3D', 'Y_3D', 'Z_3D']].values[0] if not row.empty else np.array([np.nan, np.nan, np.nan])\n",
    "        left_hip_3d = get_coords_3d('left_hip')\n",
    "        left_knee_3d = get_coords_3d('left_knee')\n",
    "        left_ankle_3d = get_coords_3d('left_ankle')\n",
    "        right_hip_3d = get_coords_3d('right_hip')\n",
    "        right_knee_3d = get_coords_3d('right_knee')\n",
    "        right_ankle_3d = get_coords_3d('right_ankle')\n",
    "        spine_3d = get_coords_3d('spine1')\n",
    "        def angle_between_3d(v1, v2):\n",
    "            if np.any(np.isnan(v1)) or np.any(np.isnan(v2)):\n",
    "                return np.nan\n",
    "            cos_theta = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "            return np.degrees(np.arccos(np.clip(cos_theta, -1.0, 1.0)))\n",
    "        angles_3d['left_knee'].append(angle_between_3d(left_hip_3d - left_knee_3d, left_ankle_3d - left_knee_3d))\n",
    "        angles_3d['right_knee'].append(angle_between_3d(right_hip_3d - right_knee_3d, right_ankle_3d - right_knee_3d))\n",
    "        angles_3d['left_hip'].append(angle_between_3d(spine_3d - left_hip_3d, left_knee_3d - left_hip_3d))\n",
    "        angles_3d['right_hip'].append(angle_between_3d(spine_3d - right_hip_3d, right_knee_3d - right_hip_3d))\n",
    "    avg_angles_3d = {k: np.nanmean(v) for k, v in angles_3d.items()}\n",
    "    return {\n",
    "        'Person': person_id,\n",
    "        'Stride Time Left (s)': stride_time_left_3d,\n",
    "        'Stride Time Right (s)': stride_time_right_3d,\n",
    "        'Stride Length Left (mm)': stride_length_left_3d,\n",
    "        'Stride Length Right (mm)': stride_length_right_3d,\n",
    "        'Cadence (steps/min)': cadence_3d,\n",
    "        'Gait Speed Left (mm/s)': gait_speed_left_3d,\n",
    "        'Gait Speed Right (mm/s)': gait_speed_right_3d,\n",
    "        'Avg Left Knee Angle (deg)': avg_angles_3d['left_knee'],\n",
    "        'Avg Right Knee Angle (deg)': avg_angles_3d['right_knee'],\n",
    "        'Avg Left Hip Angle (deg)': avg_angles_3d['left_hip'],\n",
    "        'Avg Right Hip Angle (deg)': avg_angles_3d['right_hip']\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    zip_path = \"C:\\\\Users\\\\akhileshsing2024\\\\Downloads\\\\metrabs_mob3l_y4t.zip\"\n",
    "    video_path = \"C:\\\\Users\\\\akhileshsing2024\\\\Downloads\\\\Mediapipe\\\\walk.mp4\"\n",
    "    output_excel = \"2dand3dposedatawithlabelschanged.xlsx\"\n",
    "    \n",
    "    model_path = extract_model(zip_path)\n",
    "    model = tf.saved_model.load(model_path)\n",
    "    print(\"Model loaded successfully!\")\n",
    "    \n",
    "    skeleton = 'smpl_24'\n",
    "    joint_names = model.per_skeleton_joint_names[skeleton].numpy().astype(str)\n",
    "    joint_edges = model.per_skeleton_joint_edges[skeleton].numpy()\n",
    "    \n",
    "    df = process_video(video_path, model, joint_edges, output_excel=output_excel)\n",
    "    \n",
    "    unique_persons = df['Person'].unique()\n",
    "    pixel_to_mm_factors = []\n",
    "    for person_id in unique_persons:\n",
    "        df_person = df[df['Person'] == person_id]\n",
    "        left_hip_2d = df_person[df_person['Joint'] == 1][['Frame', 'X_2D', 'Y_2D']]\n",
    "        left_ankle_2d = df_person[df_person['Joint'] == 7][['Frame', 'X_2D', 'Y_2D']]\n",
    "        left_hip_3d = df_person[df_person['Joint'] == 1][['Frame', 'X_3D', 'Y_3D', 'Z_3D']]\n",
    "        left_ankle_3d = df_person[df_person['Joint'] == 7][['Frame', 'X_3D', 'Y_3D', 'Z_3D']]\n",
    "        hip_ankle_2d = left_hip_2d.merge(left_ankle_2d, on='Frame', suffixes=('_hip', '_ankle'))\n",
    "        hip_ankle_3d = left_hip_3d.merge(left_ankle_3d, on='Frame', suffixes=('_hip', '_ankle'))\n",
    "        distances_2d = []\n",
    "        distances_3d = []\n",
    "        for idx in range(len(hip_ankle_2d)):\n",
    "            hip_2d = hip_ankle_2d.iloc[idx][['X_2D_hip', 'Y_2D_hip']].values\n",
    "            ankle_2d = hip_ankle_2d.iloc[idx][['X_2D_ankle', 'Y_2D_ankle']].values\n",
    "            dist_2d = distance.euclidean(hip_2d, ankle_2d)\n",
    "            hip_3d = hip_ankle_3d.iloc[idx][['X_3D_hip', 'Y_3D_hip', 'Z_3D_hip']].values\n",
    "            ankle_3d = hip_ankle_3d.iloc[idx][['X_3D_ankle', 'Y_3D_ankle', 'Z_3D_ankle']].values\n",
    "            dist_3d = distance.euclidean(hip_3d, ankle_3d)\n",
    "            if dist_2d > 0:\n",
    "                pixel_to_mm = dist_3d / dist_2d\n",
    "                pixel_to_mm_factors.append(pixel_to_mm)\n",
    "                distances_2d.append(dist_2d)\n",
    "                distances_3d.append(dist_3d)\n",
    "    pixel_to_mm = np.mean(pixel_to_mm_factors) if pixel_to_mm_factors else 900 / 500\n",
    "    print(f\"Computed pixel-to-mm conversion factor: {pixel_to_mm:.4f} mm/pixel\")\n",
    "    \n",
    "    df['X_2D'] = df['X_2D'] * pixel_to_mm\n",
    "    df['Y_2D'] = df['Y_2D'] * pixel_to_mm\n",
    "    updated_excel = \"2dand3dposedatawithlabelschanged_converted.xlsx\"\n",
    "    df.to_excel(updated_excel, index=False)\n",
    "    print(f\"Updated pose data saved as {updated_excel}\")\n",
    "    \n",
    "    fps = cv2.VideoCapture(video_path).get(cv2.CAP_PROP_FPS)\n",
    "    metrics_2d_all = []\n",
    "    metrics_3d_all = []\n",
    "    for person_id in unique_persons:\n",
    "        metrics_2d = calculate_gait_metrics_2d(df, person_id, fps, joint_edges, joint_names)\n",
    "        if metrics_2d:\n",
    "            metrics_2d_all.append(metrics_2d)\n",
    "        metrics_3d = calculate_gait_metrics_3d(df, person_id, fps, joint_edges, joint_names)\n",
    "        if metrics_3d:\n",
    "            metrics_3d_all.append(metrics_3d)\n",
    "    \n",
    "    metrics_df_2d = pd.DataFrame(metrics_2d_all)\n",
    "    metrics_df_3d = pd.DataFrame(metrics_3d_all)\n",
    "    with pd.ExcelWriter(\"gait_metrics_per_person.xlsx\") as writer:\n",
    "        metrics_df_2d.to_excel(writer, sheet_name='2D Metrics', index=False)\n",
    "        metrics_df_3d.to_excel(writer, sheet_name='3D Metrics', index=False)\n",
    "    print(\"Gait metrics saved to 'gait_metrics_per_person.xlsx'\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2b217b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metrabs_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
