{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba839ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 529\u001b[0m\n\u001b[0;32m    525\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mGait metrics for all detected persons saved to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgait_metrics_per_person.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 529\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 428\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    425\u001b[0m joint_edges \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mper_skeleton_joint_edges[skeleton]\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    427\u001b[0m \u001b[38;5;66;03m# Process video (this generates the initial Excel file with 2D in pixels, 3D in mm)\u001b[39;00m\n\u001b[1;32m--> 428\u001b[0m \u001b[43mprocess_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoint_edges\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_excel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_excel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;66;03m# Load the generated Excel file\u001b[39;00m\n\u001b[0;32m    431\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(output_excel)\n",
      "Cell \u001b[1;32mIn[2], line 116\u001b[0m, in \u001b[0;36mprocess_video\u001b[1;34m(video_path, model, skeleton_edges, output_video, frame_interval, output_excel)\u001b[0m\n\u001b[0;32m    113\u001b[0m image_tensor \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(frame_rgb_resized, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39muint8)\n\u001b[0;32m    115\u001b[0m \u001b[38;5;66;03m# Run pose estimation\u001b[39;00m\n\u001b[1;32m--> 116\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect_poses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskeleton\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msmpl_24\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mposes2d\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m pred \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mposes3d\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m pred:\n\u001b[0;32m    119\u001b[0m     keypoints_2d \u001b[38;5;241m=\u001b[39m pred[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mposes2d\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Users\\akhileshsing2024\\AppData\\Local\\anaconda3\\envs\\metrabs_env\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\akhileshsing2024\\AppData\\Local\\anaconda3\\envs\\metrabs_env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\akhileshsing2024\\AppData\\Local\\anaconda3\\envs\\metrabs_env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\akhileshsing2024\\AppData\\Local\\anaconda3\\envs\\metrabs_env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\akhileshsing2024\\AppData\\Local\\anaconda3\\envs\\metrabs_env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\akhileshsing2024\\AppData\\Local\\anaconda3\\envs\\metrabs_env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\akhileshsing2024\\AppData\\Local\\anaconda3\\envs\\metrabs_env\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\akhileshsing2024\\AppData\\Local\\anaconda3\\envs\\metrabs_env\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1688\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1686\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1688\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1689\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1690\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1691\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1692\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1693\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1694\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1695\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1696\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1697\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1698\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1702\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1703\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\akhileshsing2024\\AppData\\Local\\anaconda3\\envs\\metrabs_env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import urllib.request\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as tfhub\n",
    "import tensorflow_io as tfio\n",
    "import cv2\n",
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.spatial import distance\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "\n",
    "def extract_model(zip_path, extract_to=\"models\"):\n",
    "    \"\"\"Extracts a zip file and returns the extracted directory path.\"\"\"\n",
    "    extract_dir = os.path.join(os.path.dirname(zip_path), extract_to)\n",
    "\n",
    "    if not os.path.exists(extract_dir):\n",
    "        os.makedirs(extract_dir)\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "\n",
    "    model_dir = os.path.join(extract_dir, os.path.splitext(os.path.basename(zip_path))[0])\n",
    "    return model_dir\n",
    "\n",
    "\n",
    "def draw_skeleton(frame, keypoints, skeleton_edges):\n",
    "    \"\"\"Draws skeleton on the frame using detected keypoints.\"\"\"\n",
    "    for (i, j) in skeleton_edges:\n",
    "        pt1, pt2 = keypoints[i], keypoints[j]\n",
    "\n",
    "        # Check if confidence is available (shape [N, 24, 3] expected)\n",
    "        if keypoints.shape[-1] == 3:  \n",
    "            if (pt1[2] > 0.0) and (pt2[2] > 0.0):  # Confidence threshold\n",
    "                cv2.line(frame, (int(pt1[0]), int(pt1[1])), (int(pt2[0]), int(pt2[1])), (255, 0, 0), 2)  # Blue lines\n",
    "        else:  \n",
    "            # Draw without confidence check\n",
    "            cv2.line(frame, (int(pt1[0]), int(pt1[1])), (int(pt2[0]), int(pt2[1])), (255, 0, 0), 2)\n",
    "\n",
    "    return frame\n",
    "\n",
    "\n",
    "# def visualize(im, detections, poses3d, poses2d, edges, frame_count, output_2d_dir=\"plots_2d\", output_3d_dir=\"plots_3d\"):\n",
    "#     \"\"\"Visualize 2D and 3D poses with bounding boxes and save to separate folders.\"\"\"\n",
    "#     # Create output directories if they don't exist\n",
    "#     os.makedirs(output_2d_dir, exist_ok=True)\n",
    "#     os.makedirs(output_3d_dir, exist_ok=True)\n",
    "\n",
    "#     # Create figure for 2D plot\n",
    "#     fig_2d = plt.figure(figsize=(5, 5.2))\n",
    "#     image_ax = fig_2d.add_subplot(1, 1, 1)\n",
    "#     image_ax.imshow(im)\n",
    "#     # for x, y, w, h in detections[:, :4]:\n",
    "#     #     image_ax.add_patch(Rectangle((x, y), w, h, fill=False))\n",
    "\n",
    "#     for pose2d in poses2d:\n",
    "#         for i_start, i_end in edges:\n",
    "#             image_ax.plot(*zip(pose2d[i_start], pose2d[i_end]), marker='o', markersize=2)\n",
    "#         image_ax.scatter(*pose2d.T, s=2)\n",
    "\n",
    "#     # Save 2D plot\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(os.path.join(output_2d_dir, f\"frame_{frame_count}.png\"), dpi=300, bbox_inches='tight')\n",
    "#     plt.close(fig_2d)\n",
    "\n",
    "#     # Create figure for 3D plot\n",
    "#     fig_3d = plt.figure(figsize=(5, 5.2))\n",
    "#     pose_ax = fig_3d.add_subplot(1, 1, 1, projection='3d')\n",
    "#     pose_ax.view_init(5, -85)\n",
    "#     pose_ax.set_xlim3d(-1500, 1500)\n",
    "#     pose_ax.set_zlim3d(-1500, 1500)\n",
    "#     pose_ax.set_ylim3d(0, 3000)\n",
    "\n",
    "#     poses3d[..., 1], poses3d[..., 2] = poses3d[..., 2], -poses3d[..., 1]\n",
    "    \n",
    "#     for pose3d in poses3d:\n",
    "#         for i_start, i_end in edges:\n",
    "#             pose_ax.plot(*zip(pose3d[i_start], pose3d[i_end]), marker='o', markersize=2)\n",
    "#         pose_ax.scatter(*pose3d.T, s=2)\n",
    "\n",
    "#     # Save 3D plot\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(os.path.join(output_3d_dir, f\"frame_{frame_count}.png\"), dpi=300, bbox_inches='tight')\n",
    "#     plt.close(fig_3d)\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "def visualize(im, detections, poses3d, poses2d, edges, frame_count, output_2d_dir=\"plots_2d\", output_3d_dir=\"plots_3d\"):\n",
    "    \"\"\"Visualize 2D and 3D poses with bounding boxes and save to separate folders.\"\"\"\n",
    "    # Create output directories if they don't exist\n",
    "    os.makedirs(output_2d_dir, exist_ok=True)\n",
    "    os.makedirs(output_3d_dir, exist_ok=True)\n",
    "\n",
    "    # Create figure for 2D plot\n",
    "    fig_2d = plt.figure(figsize=(5, 5.2))\n",
    "    image_ax = fig_2d.add_subplot(1, 1, 1)\n",
    "    image_ax.imshow(im)\n",
    "    # for x, y, w, h in detections[:, :4]:\n",
    "    #     image_ax.add_patch(Rectangle((x, y), w, h, fill=False))\n",
    "\n",
    "    for pose2d in poses2d:\n",
    "        for i_start, i_end in edges:\n",
    "            image_ax.plot(*zip(pose2d[i_start], pose2d[i_end]), marker='o', markersize=2, linewidth=2)  # Increased linewidth to 2\n",
    "        image_ax.scatter(*pose2d.T, s=2)\n",
    "\n",
    "    # Save 2D plot\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_2d_dir, f\"frame_{frame_count}.png\"), dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig_2d)\n",
    "\n",
    "    # Create figure for 3D plot\n",
    "    fig_3d = plt.figure(figsize=(5, 5.2))\n",
    "    pose_ax = fig_3d.add_subplot(1, 1, 1, projection='3d')\n",
    "    pose_ax.view_init(5, -85)\n",
    "    pose_ax.set_xlim3d(-1500, 1500)\n",
    "    pose_ax.set_zlim3d(-1500, 1500)\n",
    "    pose_ax.set_ylim3d(0, 3000)\n",
    "\n",
    "    poses3d[..., 1], poses3d[..., 2] = poses3d[..., 2], -poses3d[..., 1]\n",
    "    \n",
    "    for pose3d in poses3d:\n",
    "        for i_start, i_end in edges:\n",
    "            pose_ax.plot(*zip(pose3d[i_start], pose3d[i_end]), marker='o', markersize=2, linewidth=2)  # Increased linewidth to 2\n",
    "        pose_ax.scatter(*pose3d.T, s=2)\n",
    "\n",
    "    # Save 3D plot\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(output_3d_dir, f\"frame_{frame_count}.png\"), dpi=300, bbox_inches='tight')\n",
    "    plt.close(fig_3d)\n",
    "\n",
    "def process_video(video_path, model, skeleton_edges, output_video=\"output_video3d.mp4\", frame_interval=1, output_excel=\"2dand3dposedatawithlabels.xlsx\"):\n",
    "    \"\"\"Process video frames with pose estimation, overlay skeletons and person labels, save 2D and 3D plots, and save coordinates to an Excel file.\"\"\"\n",
    "    \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_width = int(cap.get(3))\n",
    "    frame_height = int(cap.get(4))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    frame_count = 0\n",
    "    data_list = []  # Store pose data\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if frame_count % frame_interval == 0:\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            original_height, original_width = frame.shape[:2]\n",
    "            frame_rgb_resized = cv2.resize(frame_rgb, (640, 480))\n",
    "            image_tensor = tf.convert_to_tensor(frame_rgb_resized, dtype=tf.uint8)\n",
    "\n",
    "            # Run pose estimation\n",
    "            pred = model.detect_poses(image_tensor, skeleton='smpl_24')\n",
    "\n",
    "            if 'poses2d' in pred and 'poses3d' in pred:\n",
    "                keypoints_2d = pred['poses2d'].numpy()\n",
    "                keypoints_3d = pred['poses3d'].numpy()\n",
    "                boxes = pred.get('boxes', tf.zeros((keypoints_2d.shape[0], 4))).numpy()  # Get bounding boxes if available\n",
    "\n",
    "                if keypoints_2d.shape[0] == 0:\n",
    "                    print(f\"No pose detected in frame {frame_count}\")\n",
    "                else:\n",
    "                    # Save 2D and 3D plots\n",
    "                    visualize(frame_rgb_resized, boxes, keypoints_3d, keypoints_2d[:, :, :2], skeleton_edges, frame_count)\n",
    "\n",
    "                    # Iterate over detected poses (persons)\n",
    "                    for i in range(keypoints_2d.shape[0]):\n",
    "                        if keypoints_2d[i].shape[0] > 0 and keypoints_3d[i].shape[0] > 0:  # Check for valid keypoints\n",
    "                            kpt_2d = keypoints_2d[i]  # Get keypoints for person i\n",
    "                            kpt_3d = keypoints_3d[i]\n",
    "\n",
    "                            # Scale keypoints to original frame size\n",
    "                            kpt_2d[:, 0] *= original_width / 640  # Scale x-coordinates\n",
    "                            kpt_2d[:, 1] *= original_height / 480  # Scale y-coordinates\n",
    "\n",
    "                            # Draw skeleton\n",
    "                            frame = draw_skeleton(frame, kpt_2d, skeleton_edges)\n",
    "\n",
    "                            # Add person label (e.g., \"Person 1\", \"Person 2\", etc.)\n",
    "                            person_label = f\"Person {i + 1}\"  # Person ID starts from 1\n",
    "                            label_position = (int(kpt_2d[0][0]), int(kpt_2d[0][1] - 20))  # Position above the head\n",
    "                            cv2.putText(frame, person_label, label_position, \n",
    "                                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "                            # Store frame-wise keypoints\n",
    "                            for j in range(min(kpt_2d.shape[0], kpt_3d.shape[0])):  # Ensure we only access valid joints\n",
    "                                data_list.append([\n",
    "                                    frame_count, \n",
    "                                    i, \n",
    "                                    j, \n",
    "                                    kpt_2d[j][0], \n",
    "                                    kpt_2d[j][1], \n",
    "                                    kpt_3d[j][0], \n",
    "                                    kpt_3d[j][1], \n",
    "                                    kpt_3d[j][2],  # Include Z coordinate for 3D keypoints\n",
    "                                ])\n",
    "                        else:\n",
    "                            print(f\"Invalid keypoints for frame {frame_count}, person {i}\")\n",
    "\n",
    "        out.write(frame)  # Write frame to output video\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "\n",
    "    # Convert collected data to a Pandas DataFrame\n",
    "    df = pd.DataFrame(data_list, columns=['Frame', 'Person', 'Joint', 'X_2D', 'Y_2D', 'X_3D', 'Y_3D', 'Z_3D'])\n",
    "\n",
    "    # Save to Excel file\n",
    "    df.to_excel(output_excel, index=False)\n",
    "    print(f\"Processed video saved as {output_video}\")\n",
    "    print(f\"Pose data saved as {output_excel}\")\n",
    "\n",
    "\n",
    "def calculate_gait_metrics_2d(df, person_id, fps, skeleton_edges, joint_names):\n",
    "    \"\"\"Calculate 2D gait metrics for a specific person.\"\"\"\n",
    "    joint_map = {\n",
    "        'pelvis': 0, 'left_hip': 1, 'right_hip': 2, 'spine1': 3,\n",
    "        'left_knee': 4, 'right_knee': 5, 'spine2': 6,\n",
    "        'left_ankle': 7, 'right_ankle': 8\n",
    "    }\n",
    "\n",
    "    df_person = df[df['Person'] == person_id]\n",
    "    if df_person.empty:\n",
    "        return None  # No data for this person\n",
    "\n",
    "    left_ankle_2d = df_person[df_person['Joint'] == joint_map['left_ankle']][['Frame', 'X_2D', 'Y_2D']].values\n",
    "    right_ankle_2d = df_person[df_person['Joint'] == joint_map['right_ankle']][['Frame', 'X_2D', 'Y_2D']].values\n",
    "\n",
    "    # Detect heel strikes in 2D (lowest Y_2D position)\n",
    "    left_strikes_2d = []\n",
    "    right_strikes_2d = []\n",
    "    for i in range(1, len(left_ankle_2d) - 1):\n",
    "        if left_ankle_2d[i][2] < left_ankle_2d[i-1][2] and left_ankle_2d[i][2] < left_ankle_2d[i+1][2]:\n",
    "            left_strikes_2d.append(left_ankle_2d[i])\n",
    "        if right_ankle_2d[i][2] < right_ankle_2d[i-1][2] and right_ankle_2d[i][2] < right_ankle_2d[i+1][2]:\n",
    "            right_strikes_2d.append(right_ankle_2d[i])\n",
    "\n",
    "    left_strikes_2d = np.array(left_strikes_2d)\n",
    "    right_strikes_2d = np.array(right_strikes_2d)\n",
    "\n",
    "    # 1. Stride Time (2D)\n",
    "    stride_time_left_2d = np.mean(np.diff(left_strikes_2d[:, 0]) / fps) if len(left_strikes_2d) > 1 else np.nan\n",
    "    stride_time_right_2d = np.mean(np.diff(right_strikes_2d[:, 0]) / fps) if len(right_strikes_2d) > 1 else np.nan\n",
    "\n",
    "    # 2. Stride Length (2D, in pixels)\n",
    "    stride_length_left_2d = np.mean([distance.euclidean(left_strikes_2d[i][1:3], left_strikes_2d[i+1][1:3]) \n",
    "                                    for i in range(len(left_strikes_2d)-1)]) if len(left_strikes_2d) > 1 else np.nan\n",
    "    stride_length_right_2d = np.mean([distance.euclidean(right_strikes_2d[i][1:3], right_strikes_2d[i+1][1:3]) \n",
    "                                     for i in range(len(right_strikes_2d)-1)]) if len(right_strikes_2d) > 1 else np.nan\n",
    "\n",
    "    # 3. Cadence (2D)\n",
    "    total_steps_2d = len(left_strikes_2d) + len(right_strikes_2d)\n",
    "    total_time_2d = (df_person['Frame'].max() - df_person['Frame'].min()) / fps / 60 if not df_person['Frame'].empty else 0\n",
    "    cadence_2d = total_steps_2d / total_time_2d if total_time_2d > 0 else np.nan\n",
    "\n",
    "    # 4. Gait Speed (2D, pixels per second)\n",
    "    gait_speed_left_2d = stride_length_left_2d / stride_time_left_2d if stride_time_left_2d > 0 else np.nan\n",
    "    gait_speed_right_2d = stride_length_right_2d / stride_time_right_2d if stride_time_right_2d > 0 else np.nan\n",
    "\n",
    "    # 5. Knee and Hip Angles (2D)\n",
    "    angles_2d = {'left_knee': [], 'right_knee': [], 'left_hip': [], 'right_hip': []}\n",
    "    for frame in df_person['Frame'].unique():\n",
    "        frame_data = df_person[df_person['Frame'] == frame]\n",
    "        \n",
    "        def get_coords_2d(joint):\n",
    "            row = frame_data[frame_data['Joint'] == joint_map[joint]]\n",
    "            return row[['X_2D', 'Y_2D']].values[0] if not row.empty else np.array([np.nan, np.nan])\n",
    "\n",
    "        left_hip_2d = get_coords_2d('left_hip')\n",
    "        left_knee_2d = get_coords_2d('left_knee')\n",
    "        left_ankle_2d = get_coords_2d('left_ankle')\n",
    "        right_hip_2d = get_coords_2d('right_hip')\n",
    "        right_knee_2d = get_coords_2d('right_knee')\n",
    "        right_ankle_2d = get_coords_2d('right_ankle')\n",
    "        spine_2d = get_coords_2d('spine1')\n",
    "\n",
    "        def angle_between_2d(v1, v2):\n",
    "            if np.any(np.isnan(v1)) or np.any(np.isnan(v2)):\n",
    "                return np.nan\n",
    "            cos_theta = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "            return np.degrees(np.arccos(np.clip(cos_theta, -1.0, 1.0)))\n",
    "\n",
    "        angles_2d['left_knee'].append(angle_between_2d(left_hip_2d - left_knee_2d, left_ankle_2d - left_knee_2d))\n",
    "        angles_2d['right_knee'].append(angle_between_2d(right_hip_2d - right_knee_2d, right_ankle_2d - right_knee_2d))\n",
    "        angles_2d['left_hip'].append(angle_between_2d(spine_2d - left_hip_2d, left_knee_2d - left_hip_2d))\n",
    "        angles_2d['right_hip'].append(angle_between_2d(spine_2d - right_hip_2d, right_knee_2d - right_hip_2d))\n",
    "\n",
    "    avg_angles_2d = {k: np.nanmean(v) for k, v in angles_2d.items()}\n",
    "\n",
    "    return {\n",
    "        'Person': person_id,\n",
    "        'Stride Time Left (s)': stride_time_left_2d,\n",
    "        'Stride Time Right (s)': stride_time_right_2d,\n",
    "        'Stride Length Left (mm)': stride_length_left_2d,\n",
    "        'Stride Length Right (mm)': stride_length_right_2d,\n",
    "        'Cadence (steps/min)': cadence_2d,\n",
    "        'Gait Speed Left (mm/s)': gait_speed_left_2d,\n",
    "        'Gait Speed Right (mm/s)': gait_speed_right_2d,\n",
    "        'Avg Left Knee Angle (deg)': avg_angles_2d['left_knee'],\n",
    "        'Avg Right Knee Angle (deg)': avg_angles_2d['right_knee'],\n",
    "        'Avg Left Hip Angle (deg)': avg_angles_2d['left_hip'],\n",
    "        'Avg Right Hip Angle (deg)': avg_angles_2d['right_hip']\n",
    "    }\n",
    "\n",
    "\n",
    "def calculate_gait_metrics_3d(df, person_id, fps, skeleton_edges, joint_names):\n",
    "    \"\"\"Calculate 3D gait metrics for a specific person.\"\"\"\n",
    "    joint_map = {\n",
    "        'pelvis': 0, 'left_hip': 1, 'right_hip': 2, 'spine1': 3,\n",
    "        'left_knee': 4, 'right_knee': 5, 'spine2': 6,\n",
    "        'left_ankle': 7, 'right_ankle': 8\n",
    "    }\n",
    "\n",
    "    df_person = df[df['Person'] == person_id]\n",
    "    if df_person.empty:\n",
    "        return None  # No data for this person\n",
    "\n",
    "    left_ankle_3d = df_person[df_person['Joint'] == joint_map['left_ankle']][['Frame', 'X_3D', 'Y_3D', 'Z_3D']].values\n",
    "    right_ankle_3d = df_person[df_person['Joint'] == joint_map['right_ankle']][['Frame', 'X_3D', 'Y_3D', 'Z_3D']].values\n",
    "\n",
    "    # Detect heel strikes in 3D (lowest Z_3D position)\n",
    "    left_strikes_3d = []\n",
    "    right_strikes_3d = []\n",
    "    for i in range(1, len(left_ankle_3d) - 1):\n",
    "        if left_ankle_3d[i][3] < left_ankle_3d[i-1][3] and left_ankle_3d[i][3] < left_ankle_3d[i+1][3]:\n",
    "            left_strikes_3d.append(left_ankle_3d[i])\n",
    "        if right_ankle_3d[i][3] < right_ankle_3d[i-1][3] and right_ankle_3d[i][3] < right_ankle_3d[i+1][3]:\n",
    "            right_strikes_3d.append(right_ankle_3d[i])\n",
    "\n",
    "    left_strikes_3d = np.array(left_strikes_3d)\n",
    "    right_strikes_3d = np.array(right_strikes_3d)\n",
    "\n",
    "    # 1. Stride Time (3D)\n",
    "    stride_time_left_3d = np.mean(np.diff(left_strikes_3d[:, 0]) / fps) if len(left_strikes_3d) > 1 else np.nan\n",
    "    stride_time_right_3d = np.mean(np.diff(right_strikes_3d[:, 0]) / fps) if len(right_strikes_3d) > 1 else np.nan\n",
    "\n",
    "    # 2. Stride Length (3D, assumed meters)\n",
    "    stride_length_left_3d = np.mean([distance.euclidean(left_strikes_3d[i][1:4], left_strikes_3d[i+1][1:4]) \n",
    "                                    for i in range(len(left_strikes_3d)-1)]) if len(left_strikes_3d) > 1 else np.nan\n",
    "    stride_length_right_3d = np.mean([distance.euclidean(right_strikes_3d[i][1:4], right_strikes_3d[i+1][1:4]) \n",
    "                                     for i in range(len(right_strikes_3d)-1)]) if len(right_strikes_3d) > 1 else np.nan\n",
    "\n",
    "    # 3. Cadence (3D)\n",
    "    total_steps_3d = len(left_strikes_3d) + len(right_strikes_3d)\n",
    "    total_time_3d = (df_person['Frame'].max() - df_person['Frame'].min()) / fps / 60 if not df_person['Frame'].empty else 0\n",
    "    cadence_3d = total_steps_3d / total_time_3d if total_time_3d > 0 else np.nan\n",
    "\n",
    "    # 4. Gait Speed (3D, assumed meters per second)\n",
    "    gait_speed_left_3d = stride_length_left_3d / stride_time_left_3d if stride_time_left_3d > 0 else np.nan\n",
    "    gait_speed_right_3d = stride_length_right_3d / stride_time_right_3d if stride_time_right_3d > 0 else np.nan\n",
    "\n",
    "    # 5. Knee and Hip Angles (3D)\n",
    "    angles_3d = {'left_knee': [], 'right_knee': [], 'left_hip': [], 'right_hip': []}\n",
    "    for frame in df_person['Frame'].unique():\n",
    "        frame_data = df_person[df_person['Frame'] == frame]\n",
    "        \n",
    "        def get_coords_3d(joint):\n",
    "            row = frame_data[frame_data['Joint'] == joint_map[joint]]\n",
    "            return row[['X_3D', 'Y_3D', 'Z_3D']].values[0] if not row.empty else np.array([np.nan, np.nan, np.nan])\n",
    "\n",
    "        left_hip_3d = get_coords_3d('left_hip')\n",
    "        left_knee_3d = get_coords_3d('left_knee')\n",
    "        left_ankle_3d = get_coords_3d('left_ankle')\n",
    "        right_hip_3d = get_coords_3d('right_hip')\n",
    "        right_knee_3d = get_coords_3d('right_knee')\n",
    "        right_ankle_3d = get_coords_3d('right_ankle')\n",
    "        spine_3d = get_coords_3d('spine1')\n",
    "\n",
    "        def angle_between_3d(v1, v2):\n",
    "            if np.any(np.isnan(v1)) or np.any(np.isnan(v2)):\n",
    "                return np.nan\n",
    "            cos_theta = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "            return np.degrees(np.arccos(np.clip(cos_theta, -1.0, 1.0)))\n",
    "\n",
    "        angles_3d['left_knee'].append(angle_between_3d(left_hip_3d - left_knee_3d, left_ankle_3d - left_knee_3d))\n",
    "        angles_3d['right_knee'].append(angle_between_3d(right_hip_3d - right_knee_3d, right_ankle_3d - right_knee_3d))\n",
    "        angles_3d['left_hip'].append(angle_between_3d(spine_3d - left_hip_3d, left_knee_3d - left_hip_3d))\n",
    "        angles_3d['right_hip'].append(angle_between_3d(spine_3d - right_hip_3d, right_knee_3d - right_hip_3d))\n",
    "\n",
    "    avg_angles_3d = {k: np.nanmean(v) for k, v in angles_3d.items()}\n",
    "\n",
    "    return {\n",
    "        'Person': person_id,\n",
    "        'Stride Time Left (s)': stride_time_left_3d,\n",
    "        'Stride Time Right (s)': stride_time_right_3d,\n",
    "        'Stride Length Left (mm)': stride_length_left_3d,\n",
    "        'Stride Length Right (mm)': stride_length_right_3d,\n",
    "        'Cadence (steps/min)': cadence_3d,\n",
    "        'Gait Speed Left (mm/s)': gait_speed_left_3d,\n",
    "        'Gait Speed Right (mm/s)': gait_speed_right_3d,\n",
    "        'Avg Left Knee Angle (deg)': avg_angles_3d['left_knee'],\n",
    "        'Avg Right Knee Angle (deg)': avg_angles_3d['right_knee'],\n",
    "        'Avg Left Hip Angle (deg)': avg_angles_3d['left_hip'],\n",
    "        'Avg Right Hip Angle (deg)': avg_angles_3d['right_hip']\n",
    "    }\n",
    "\n",
    "def plot_ankle_z_data(df, output_dir=\"plots_ankle_z\"):\n",
    "    \"\"\"Plot Z-direction data for left and right ankles for each person.\"\"\"\n",
    "    # Create output directory for ankle Z plots\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Joint map for ankle indices\n",
    "    joint_map = {\n",
    "        'left_ankle': 7,\n",
    "        'right_ankle': 8\n",
    "    }\n",
    "    \n",
    "    # Get unique person IDs\n",
    "    unique_persons = df['Person'].unique()\n",
    "    \n",
    "    for person_id in unique_persons:\n",
    "        # Filter data for this person\n",
    "        df_person = df[df['Person'] == person_id]\n",
    "        \n",
    "        # Extract left and right ankle Z-coordinates\n",
    "        left_ankle_z = df_person[df_person['Joint'] == joint_map['left_ankle']][['Frame', 'Z_3D']].sort_values('Frame')\n",
    "        right_ankle_z = df_person[df_person['Joint'] == joint_map['right_ankle']][['Frame', 'Z_3D']].sort_values('Frame')\n",
    "        \n",
    "        # Skip if no data for ankles\n",
    "        if left_ankle_z.empty and right_ankle_z.empty:\n",
    "            print(f\"No ankle data for Person {person_id}\")\n",
    "            continue\n",
    "        \n",
    "        # Create plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Plot left ankle Z-coordinate\n",
    "        if not left_ankle_z.empty:\n",
    "            plt.plot(left_ankle_z['Frame'], left_ankle_z['Z_3D'], label=f'Person {person_id} Left Ankle', color='blue', linestyle='-')\n",
    "        \n",
    "        # Plot right ankle Z-coordinate\n",
    "        if not right_ankle_z.empty:\n",
    "            plt.plot(right_ankle_z['Frame'], right_ankle_z['Z_3D'], label=f'Person {person_id} Right Ankle', color='red', linestyle='--')\n",
    "        \n",
    "        # Customize plot\n",
    "        plt.xlabel('Frame')\n",
    "        plt.ylabel('Z-Coordinate (mm)')\n",
    "        plt.title(f'Ankle Z-Coordinate Movement for Person {person_id}')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Save plot\n",
    "        plt.savefig(os.path.join(output_dir, f'person_{person_id}_ankle_z.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Saved ankle Z plot for Person {person_id} to {os.path.join(output_dir, f'person_{person_id}_ankle_z.png')}\")\n",
    "def main():\n",
    "    # Paths\n",
    "    zip_path = r\"C:\\\\Users\\\\akhileshsing2024\\\\Downloads\\\\metrabs_eff2l_y4_360.zip\"\n",
    "    video_path = r\"C:\\Users\\akhileshsing2024\\Desktop\\Curve_walking_No_task\\IMG_0597.MOV\"\n",
    "    output_excel = \"2dand3dposedatawithlabels.xlsx\"\n",
    "\n",
    "    # Extract and load the model\n",
    "    model_path = extract_model(zip_path)\n",
    "    model = tf.saved_model.load(model_path)\n",
    "    print(\"Model loaded successfully!\")\n",
    "\n",
    "    # Load skeleton information\n",
    "    skeleton = 'smpl_24'\n",
    "    joint_names = model.per_skeleton_joint_names[skeleton].numpy().astype(str)\n",
    "    joint_edges = model.per_skeleton_joint_edges[skeleton].numpy()\n",
    "\n",
    "    # Process video (this generates the initial Excel file with 2D in pixels, 3D in mm)\n",
    "    process_video(video_path, model, joint_edges, output_excel=output_excel)\n",
    "\n",
    "    # Load the generated Excel file\n",
    "    df = pd.read_excel(output_excel)\n",
    "\n",
    "    # Step 1: Compute pixel-to-mm conversion factor using hip-to-ankle distance\n",
    "    unique_persons = df['Person'].unique()\n",
    "    pixel_to_mm_factors = []\n",
    "\n",
    "    for person_id in unique_persons:\n",
    "        df_person = df[df['Person'] == person_id]\n",
    "        \n",
    "        # Extract hip and ankle coordinates for all frames\n",
    "        left_hip_2d = df_person[df_person['Joint'] == 1][['Frame', 'X_2D', 'Y_2D']]\n",
    "        left_ankle_2d = df_person[df_person['Joint'] == 7][['Frame', 'X_2D', 'Y_2D']]\n",
    "        left_hip_3d = df_person[df_person['Joint'] == 1][['Frame', 'X_3D', 'Y_3D', 'Z_3D']]\n",
    "        left_ankle_3d = df_person[df_person['Joint'] == 7][['Frame', 'X_3D', 'Y_3D', 'Z_3D']]\n",
    "\n",
    "        # Merge on Frame to get corresponding hip and ankle coordinates\n",
    "        hip_ankle_2d = left_hip_2d.merge(left_ankle_2d, on='Frame', suffixes=('_hip', '_ankle'))\n",
    "        hip_ankle_3d = left_hip_3d.merge(left_ankle_3d, on='Frame', suffixes=('_hip', '_ankle'))\n",
    "\n",
    "        # Compute distances for each frame\n",
    "        distances_2d = []\n",
    "        distances_3d = []\n",
    "        for idx in range(len(hip_ankle_2d)):\n",
    "            # 2D distance (pixels)\n",
    "            hip_2d = hip_ankle_2d.iloc[idx][['X_2D_hip', 'Y_2D_hip']].values\n",
    "            ankle_2d = hip_ankle_2d.iloc[idx][['X_2D_ankle', 'Y_2D_ankle']].values\n",
    "            dist_2d = distance.euclidean(hip_2d, ankle_2d)\n",
    "\n",
    "            # 3D distance (mm)\n",
    "            hip_3d = hip_ankle_3d.iloc[idx][['X_3D_hip', 'Y_3D_hip', 'Z_3D_hip']].values\n",
    "            ankle_3d = hip_ankle_3d.iloc[idx][['X_3D_ankle', 'Y_3D_ankle', 'Z_3D_ankle']].values\n",
    "            dist_3d = distance.euclidean(hip_3d, ankle_3d)\n",
    "\n",
    "            if dist_2d > 0:  # Avoid division by zero\n",
    "                pixel_to_mm = dist_3d / dist_2d\n",
    "                pixel_to_mm_factors.append(pixel_to_mm)\n",
    "                distances_2d.append(dist_2d)\n",
    "                distances_3d.append(dist_3d)\n",
    "\n",
    "    # Compute the average pixel-to-mm factor\n",
    "    if pixel_to_mm_factors:\n",
    "        pixel_to_mm = np.mean(pixel_to_mm_factors)\n",
    "        print(f\"Computed pixel-to-mm conversion factor: {pixel_to_mm:.4f} mm/pixel\")\n",
    "        print(f\"Average 3D hip-to-ankle distance: {np.mean(distances_3d):.2f} mm\")\n",
    "        print(f\"Average 2D hip-to-ankle distance: {np.mean(distances_2d):.2f} pixels\")\n",
    "    else:\n",
    "        # Fallback to anthropometric average if no valid distances are computed\n",
    "        pixel_to_mm = 900 / 500  # Assume 900 mm leg length, 500 pixels in 2D (rough estimate)\n",
    "        print(f\"No valid distances computed. Using fallback pixel-to-mm factor: {pixel_to_mm:.4f} mm/pixel\")\n",
    "\n",
    "    # Step 2: Convert 2D coordinates to millimeters\n",
    "    df['X_2D'] = df['X_2D'] * pixel_to_mm\n",
    "    df['Y_2D'] = df['Y_2D'] * pixel_to_mm\n",
    "\n",
    "    # Step 3: Save the updated Excel file with 2D and 3D coordinates in millimeters\n",
    "    updated_excel = \"2dand3dposedatawithlabels_in_mm.xlsx\"\n",
    "    df.to_excel(updated_excel, index=False)\n",
    "    print(f\"Updated pose data with 2D coordinates in millimeters saved as {updated_excel}\")\n",
    "\n",
    "    plot_ankle_z_data(df)\n",
    "\n",
    "    # Step 4: Calculate gait metrics\n",
    "    fps = cv2.VideoCapture(video_path).get(cv2.CAP_PROP_FPS)\n",
    "    print(f\"Detected {len(unique_persons)} persons in the video.\")\n",
    "\n",
    "    metrics_2d_all = []\n",
    "    metrics_3d_all = []\n",
    "    for person_id in unique_persons:\n",
    "        print(f\"\\nProcessing Person {person_id}...\")\n",
    "        \n",
    "        # 2D Metrics (now in millimeters)\n",
    "        metrics_2d = calculate_gait_metrics_2d(df, person_id, fps, joint_edges, joint_names)\n",
    "        if metrics_2d:\n",
    "            metrics_2d_all.append(metrics_2d)\n",
    "            print(f\"2D Metrics for Person {person_id}:\")\n",
    "            for key, value in metrics_2d.items():\n",
    "                if key != 'Person':\n",
    "                    print(f\"{key}: {value:.2f}\" if not np.isnan(value) else f\"{key}: N/A\")\n",
    "\n",
    "        # 3D Metrics (already in millimeters)\n",
    "        metrics_3d = calculate_gait_metrics_3d(df, person_id, fps, joint_edges, joint_names)\n",
    "        if metrics_3d:\n",
    "            metrics_3d_all.append(metrics_3d)\n",
    "            print(f\"3D Metrics for Person {person_id}:\")\n",
    "            for key, value in metrics_3d.items():\n",
    "                if key != 'Person':\n",
    "                    print(f\"{key}: {value:.2f}\" if not np.isnan(value) else f\"{key}: N/A\")\n",
    "\n",
    "    # Save to Excel with separate sheets for 2D and 3D\n",
    "    metrics_df_2d = pd.DataFrame(metrics_2d_all)\n",
    "    metrics_df_3d = pd.DataFrame(metrics_3d_all)\n",
    "    with pd.ExcelWriter(\"gait_metrics_per_person.xlsx\") as writer:\n",
    "        metrics_df_2d.to_excel(writer, sheet_name='2D Metrics', index=False)\n",
    "        metrics_df_3d.to_excel(writer, sheet_name='3D Metrics', index=False)\n",
    "    print(\"\\nGait metrics for all detected persons saved to 'gait_metrics_per_person.xlsx'\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cb4d25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metrabs_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
